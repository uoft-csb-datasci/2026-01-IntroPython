{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/uoft-csb-datasci/CSBdatasci_Course_Materials/blob/main/IntroPython/CAGEF_services_slide.png?raw=true\"  width=700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python for Data Science\n",
    "\n",
    "# Lecture 03: Reading, writing, and wrangling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Name:\n",
    "\n",
    "## Student ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 0.1.0 About Introduction to Python\n",
    "\n",
    "Introduction to Python is brought to you by the **Centre for the Analysis of Genome Evolution & Function (CAGEF)** bioinformatics training initiative. This course was developed based on feedback on the needs and interests of the Department of Cell & Systems Biology and the Department of Ecology and Evolutionary Biology.\n",
    "\n",
    "The structure of this course is a code-along style so it is 100% hands on! A few hours prior to each lecture, the materials will be avaialable for download at [QUERCUS](https://q.utoronto.ca/). The teaching materials will consist of a Jupyter Lab Notebook with concepts, comments, instructions, and blank spaces that you will fill out with Python code along with the instructor. Other teaching materials include a live version of the notebook, and datasets to import into Python - when required. This learning approach will allow you to spend the time coding and not taking notes! \n",
    "\n",
    "As we go along, there will be some in-class challenge questions for you to solve either individually or in cooperation with your peers. Post lecture assessments will also be available (see syllabus for grading scheme and percentages of the final mark) through [DataCamp](https://Datacamp.com) to help cement and/or extend what you learn each week.\n",
    "\n",
    "### 0.1.1 Where is this course headed?\n",
    "\n",
    "We'll take a blank slate approach here to Python and assume that you pretty much know _nothing_ about programming. From the beginning of this course to the end, we want to get you from some potential scenarios:\n",
    "\n",
    "- A pile of data (like an excel file or tab-separated file) full of experimental observations and you don't know what to do with it. \n",
    "\n",
    "- Maybe you're manipulating large tables all in excel, making custom formulas and pivot table with graphs. Now you have to repeat similar experiments and do the analysis again. \n",
    "\n",
    "- You're generating high-throughput data and there aren't any bioinformaticians around to help you sort it out.\n",
    "\n",
    "- You heard about Python and what it _could_ do for your data analysis but don't know what that means or where to start.\n",
    "\n",
    "and get you to a point where you can:\n",
    "\n",
    "- Format your data correctly for analysis\n",
    "\n",
    "- Produce basic plots and perform exploratory analysis\n",
    "\n",
    "- Make functions and scripts for re-analysing existing or new data sets\n",
    "\n",
    "- Track your experiments in a digital notebook like Jupyter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.2 How do we get there? Step-by-step.\n",
    "\n",
    "In the first two lessons, we will talk about the basic data structures and objects in Python, get cozy with the Jupyter Notebook environment, and learn how to get help when you are stuck. Because everyone gets stuck - a lot! Then you will learn how to get your data in and out of Python, how to tidy our data (data wrangling), subset and merge data. We'll take a break from data wrangling to spend our fourth lecture learning how to generate exploratory data plots. Then we'll slide into using the power of python and programming to use flow control before visiting text manipulation techniques in lectures 5 and 6. Data cleaning and string manipulation is really the battleground of coding - getting your data into the format where you can analyse it. Lastly, we will learn to write customized functions to help scale up your analyses. <br><br>\n",
    "\n",
    "<center><img src=\"https://github.com/uoft-csb-datasci/CSBdatasci_Course_Materials/blob/main/IntroPython/Draw_an_Owl.jpg?raw=true\"  width=700></center>\n",
    "\n",
    "The structure of the class is a __code-along__ style: It is fully hands on. At the end of each lecture, the complete notes will be made available in a PDF format through the corresponding Quercus module so you don't have to spend your attention on taking notes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 0.1.3 What kind of coding style will we learn?\n",
    "\n",
    "There is no single path correct from A to B - although some paths may be more elegant, or more efficient than others. With that in mind, the emphasis in this lecture series will be on:\n",
    "\n",
    "1. **Code simplicity** - learn helpful functions that allow you to focus on understanding the basic tenets of good data wrangling (reformatting) to facilitate quick exploratory data analysis and visualization. \n",
    "2. **Code readability** - format and comment your code for yourself and others so that even those with minimal experience in R will be able to quickly grasp the overall steps in your code.\n",
    "3. **Code stability** - while the core Python code is relatively stable, behaviours of functions can still change with updates. There are well-developed packages we'll focus on for our analyses. Namely, we'll become more familiar with the `pandas` series of packages for working with tabular data. This resource is well-maintained by a large community of developers. While not always the \"fastest\" approach this additional layer can help ensure your code still runs (somewhat) smoothly later down the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 0.2.0 Lecture objectives\n",
    "\n",
    "Welcome to this third lecture in a series of seven. Today we will pick up where we left off last week with DataFrames. Now that we know some of the basics regarding this data structure, we can drill into using them to properly format your data for further analyses.\n",
    "\n",
    "At the end of this lecture we will aim to have covered the following topics:\n",
    "\n",
    "1. Wide vs. long-format data and the Tidy data philosophy.\n",
    "2. Importing Excel spreadsheets.\n",
    "3. Data wrangling.\n",
    "4. Exporting wrangled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 0.3.0 A legend for text format in Jupyter markdown\n",
    "\n",
    "`grey background` - a package, function, code, command or directory. Backticks are also use for in-line code.      \n",
    "*italics* - an important term or concept or an individual file or folder     \n",
    "**bold** - heading or a term that is being defined      \n",
    "<span style=\"color:blue\">blue text</span> - named or unnamed hyperlink\n",
    "\n",
    "`...` - Within each coding cell this will indicate an area of code that students will need to complete for the code cell to run correctly. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Blue box:</b> A key concept that is being introduced\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Yellow box:</b> Risk or caution\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Green boxes:</b> Recommended reads and resources to learn Python\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Red boxes:</b> A comprehension question which may or may not involve a coding cell. You usually find these at the end of a section. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 0.4.0 Lecture and data files used in this course\n",
    "\n",
    "### 0.4.1 Weekly Lecture and skeleton files\n",
    "\n",
    "Each week, new lesson files will appear within your JupyterHub folders. We are pulling from a GitHub repository using this [Repository git-pull link](https://jupyter.utoronto.ca/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fuoft-csb-datasci%2F2026-01-IntroPython&urlpath=tree%2F2026-01-IntroPython%2F&branch=main). Simply click on the link and it will take you to the University of Toronto JupyterHub. You will need to use your UTORid credentials to complete the login process. From there you will find each week's lecture files in the directory `/2026-01-IntroPython/Lecture_XX`. You will find a partially coded `skeleton.ipynb` file as well as all of the data files necessary to run the week's lecture.\n",
    "\n",
    "Alternatively, you can download the Jupyter Notebook (`.ipynb`) and data files from JupyterHub to your personal computer if you would like to run independently of the JupyterHub.\n",
    "\n",
    "### 0.4.2 Live-coding HTML page\n",
    "\n",
    "A live lecture version will be available at [uoft-csb-datasci.github.io](https://uoft-csb-datasci.github.io/2026-01.IntroPython/index.html) that will update as the lecture progresses. Be sure to refresh to take a look if you get lost!\n",
    "\n",
    "### 0.4.3 Post-lecture PDFs\n",
    "\n",
    "As mentioned above, at the end of each lecture there will be a completed version of the lecture code released as a PDF file under the Modules section of Quercus.\n",
    "\n",
    "***\n",
    "### 0.4.4 Microsporidia infection data set description\n",
    "\n",
    "The following datasets used in this week's class come from a published manuscript on PLoS Pathogens entitled \"High-throughput phenotyping of infection by diverse microsporidia species reveals a wild _C. elegans_ strain with opposing resistance and susceptibility traits\" by [Mok et al., 2023](https://journals.plos.org/plospathogens/article?id=10.1371/journal.ppat.1011225). These datasets focus on the an analysis of infection in wild isolate strains of the nematode _C. elegans_ by environmental pathogens known as microsporidia. The authors collected embryo counts from individual animals in the population after population-wide infection by microsporidia and we'll spend our next few classes working with the dataset to learn how to format and manipulate it. \n",
    "\n",
    "### 0.4.4.1 Dataset 1: /data/infection_data_all.xlsx\n",
    "\n",
    "This is a series of amalgamated datasets that we will use to show how we can import even entire Excel books into R. This file contains two sheets containing experimental measurements as well as the experimental metadata from Dataset 1.\n",
    "\n",
    "### 0.4.4.2 Dataset 2: /data/infection_meta.csv\n",
    "\n",
    "This is a comma-separated version of the metadata data from our measurements. This dataset tracks information for each experimental condition measured including variables including experimental dates, reagent versions, and sample locations. We'll use this file to ease our way into importing, manipulating, and exporting in today's class.\n",
    "\n",
    "### 0.4.4.3 Dataset 3: /data/embryo_data_wide.csv\n",
    "\n",
    "This is a comma-separated version of measurements from a series of experiments where _C. elegans_ strains are infected under different conditions with one of many possible _Nematocida_ microsporidia species. Each column name carries unique identifiers for each experiment as well as the type of value measured while the values represent the presence of mature __`spores`__ appearing after spore infection/replication (1 = yes, 0 = no), __`meronts`__ or immature spores (1 = yes, 0 = no), and the number of __`embryos`__ present in each animal observed. Thus each grouping of 3 columns represents the 3 measurement types from a single experimental replicate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 0.5.0 Packages used in this lesson\n",
    "\n",
    "`IPython` and `InteractiveShell` will be access just to set the behaviour we want for iPython so we can see multiple code outputs per code cell.\n",
    "\n",
    "`random` is a package with methods to add pseudorandomness to programs \n",
    "\n",
    "`numpy` provides a number of mathematical functions as well as the special data class of `arrays` which we'll be learning about today.\n",
    "\n",
    "`os`\n",
    "`pandas`\n",
    "`matplolib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Always run this at the beginning of class so we can get multi-command output ----- #\n",
    "\n",
    "# Access options from the iPython core\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# Change the value of ast_node_interactivity\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# ----- Packages we want to install for class ----- #\n",
    "# !pip install openpyxl     # This should already be installed\n",
    "!pip install xlsxwriter\n",
    "\n",
    "# ----- Additional packages we want to import for class ----- #\n",
    "\n",
    "# Import the pandas package\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1.0.0 The long and the wide of it - formatting your data for analysis\n",
    "\n",
    "### 1.0.1 Defining our data terms\n",
    "\n",
    "- **Variable:** A part of an experiment that can be controlled, changed, or measured. Also referred to as \"features\" or \"dimensions\".\n",
    "\n",
    "\n",
    "- **Observation:** The results of measuring the variables of interest in an experiment.\n",
    "\n",
    "\n",
    "- **Long format:** Data format in which each column records a single variable's values and each row is an observation for those variables at a single timepoint in your experiment. This formatting philosophy is also called \"tidy data\".\n",
    "\n",
    "\n",
    "## 1.1.0 Wide versus long format\n",
    "\n",
    "Wide and long (sometimes un-stacked and stacked, or wide and tall, wide and narrow), are terms used to describe how a dataset is formatted.\n",
    "\n",
    "In a long formatted dataset, each column is a variable and the results of each measured variable are stored in rows (observations). In contrast, not every column in wide formatted data is necessarily a variable so you can have several observations of the same type of variable in the same row. The names long and wide come from the general shape that the two data formats have.\n",
    "\n",
    "For data science applications, long format is preferred over wide format because it allows for easier and more efficient computations, data subsetting and manipulation. Wide format is more friendly to the human eye and easier to work with when data needs to be manually recorded/input. Therefore, having the ability to interconvert between these two data formats is a valuable and required skill. The following is a general scheme of wide- (left) and long-format (right) datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://github.com/uoft-csb-datasci/CSBdatasci_Course_Materials/blob/main/IntroPython/wide_and_long_formats.png?raw=true\"  width=700>|\n",
    "|:--:|\n",
    "|While more readable and technically more compact, the wide data format is not easily parsed for data analysis compared to the long data format.|\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Why can't computers think like we do?</b> To read more about wide and long formats, visit <a href=\"https://eagereyes.org/basics/spreadsheet-thinking-vs-database-thinking\">this blog post</a> on spreadsheet thinking versus database thinking.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## 1.2.0 The tidy data philosophy\n",
    "\n",
    "_Why tidy data?_\n",
    "\n",
    "Data cleaning (or dealing with 'messy' data, aka ___wrangling___) accounts for a huge chunk of a data scientist's time. Ultimately, we want to get our data into a 'tidy' format (long format) where it is easy to manipulate, model and visualize. Having a consistent data structure and tools that work with that data structure can help this process along.\n",
    "\n",
    "In Tidy data:\n",
    "\n",
    "1. Each variable forms a column.\n",
    "2. Each observation forms a row.\n",
    "3. Each type of observational unit forms a table.\n",
    "\n",
    "This seems pretty straight forward, and it is. The datasets you get, however, will not be straightforward. Having a map of where to take your data is helpful in unraveling its structure and getting it into a usable format.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Observational units:</b> Of the three rules, the idea of observational units might be the hardest to grasp. As an example, you may be tracking a puppy population across 4 variables: age, height, weight, and fur colour. Each observation unit is a puppy. However, you might be tracking these puppies across multiple measurements - so a time factor applies. In that case, the observation unit now becomes puppy-time. In that case, each puppy-time measurement belongs in a different table (at least by tidy data standards). This, however, is a simple example and things can get more complex when taking into consideration what defines an observational unit. Check out this blog post by <a href=\"https://clauswilke.com/blog/2014/07/21/keep-your-data-tidy-part-ii/\">Claus O. Wilke</a> for a little more explanation.\n",
    "</div>\n",
    "\n",
    "The 5 most common problems with messy datasets are:\n",
    "\n",
    "- common headers are values, not variable names.\n",
    "- multiple variables are stored in one column.\n",
    "- variables are stored in both rows and columns.\n",
    "- a single variable stored across multiple tables.\n",
    "- multiple types of observational units stored in the same table.\n",
    "\n",
    "Fortunately there are some tools within the `Pandas` package to solve these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1.3.0 Reading in data\n",
    "\n",
    "### 1.3.1 Looking closer at our data\n",
    "\n",
    "In our case, we will create variables representing two datasets ready in our `data` folder to import for this lecture:\n",
    "\n",
    "### 1.3.1.1 `embryo_metadata` \n",
    "\n",
    "- made from importing `data/infection_meta.csv`\n",
    "\n",
    "- A comma-separated version of the metadata data from our measurements in `embryo_wide.csv`. This dataset tracks information for each experimental condition measured including variables like experimental dates, reagent versions, and sample locations. In total there are 276 rows and 29 columns such as:\n",
    "\n",
    "    - `experiment`: A unique identifier holding multiple pieces of underscore-separated information including sample collection date, worm strain, pathogen strain, pathogen dose, and experimental timepoint.\n",
    "    - `Infection Date`: The date an infection was initiated\n",
    "    - `Spore Lot`: used for tracking which batches of prepared pathogen were used for each infection experiment.\n",
    "\n",
    "### 1.3.1.2 `embryo_wide` \n",
    "\n",
    "- made from importing `data/embryo_data_wide.csv`\n",
    "\n",
    "- A comma-separated version of measurements from a series of experiments where _C. elegans_ strains are infected under different conditions with one of many possible _Nematocida_ microsporidia species. Each column name carries unique identifiers for each experiment AND an identifier for the kind of measurement taken.\n",
    "\n",
    "- The three kinds of measurements are `sporesPresent`, `merontsPresent` and `numEmbryos`. The first type represents the observation of mature __`spores`__ appearing after spore infection/replication (1 = yes, 0 = no). The second type represents the observation of __`meronts`__ or immature spores (1 = yes, 0 = no), and the third type of column is the number of __`embryos`__ present in each animal observed. In total there are 154 rows and 900 columns including:\n",
    "\n",
    "    - `worm.number`: the worm identifier used to denote which worm in the population was measured\n",
    "    - `Columns 2-901`: experiment ___identifiers___ whose names hold multiple pieces of underscore-separated information including sample collection date, worm strain, pathogen strain, pathogen dose, experimental timepoint, and measurement observed. The values in these columns hold the measured data regarding spore presence, meront presence, and embryo counts for an individual worm.\n",
    " \n",
    "|<img src=\"https://github.com/uoft-csb-datasci/CSBdatasci_Course_Materials/blob/main/IntroPython/EmbryoWideToLong.png?raw=true\" width=700>|\n",
    "|:--:|\n",
    "|Our goal with this dataset will be to convert it from it's current wide format to a long format!| \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 1.3.2 Our data-wrangling goal\n",
    "\n",
    "|<img src=\"https://github.com/uoft-csb-datasci/CSBdatasci_Course_Materials/blob/main/IntroPython/tell-me-more-about-your-data-wrangling-course.jpg?raw=true\" width=500>|\n",
    "|:--:|\n",
    "|More than 50% of your time may be spent dealing with the shape and format of your data. Once that is figured out, the analysis can seem rather simple!| \n",
    "\n",
    "We'll begin by learning how to import different data formats. Next we will use `Pandas` to introduce concepts of data wrangling to convert out datasets from wide to long formats. When we are ready, we will merge the two files into a single dataset that we can use for exploratory data analysis.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Data vs metadata?</b> Sometimes the idea can be confusing when thinking about data vs metadata. Both sets are considered data but the former usually refers to measurements that may be raw or unfiltered. These sets of measurements themselves may contain metadata information about their samples. Metadata on the other hand is always processed in some way with information that is relevant to and <i>describes</i> the measured data. Simply put - data provides content, while metadata provides context. For a brief comparison check out <u><a href=\"https://techdifferences.com/difference-between-data-and-metadata.html\">these</a></u> brief <u><a href=\"https://zeenea.com/the-difference-between-metadata-data/#:~:text=The%20main%20difference%20between%20Data,more%20context%20for%20data%20users.\">descriptions</a></u>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2.0.0 Working with Excel spreadsheets\n",
    "\n",
    "Up until this lecture any data we have used has been generated by directly inputting it as part of our code. It's now time to look at how we can __import__ data from outside sources like `txt`, `csv`, or even `xlsx` files. \n",
    "\n",
    "Given the popularity of __Excel__, it is common to find ourselves working with Excel books. Before importing the file, we need to know our current working directory (where in the computer are we working from?), change the directory if necessary, and list the files available for import. For those purposes we will use the built-in `os` library.\n",
    "\n",
    "## 2.1.0 The `os` package provides access to the <u>o</u>perating <u>s</u>ystem\n",
    "\n",
    "In order to access functions and files in your operating system, the `os` package provides utilities that facilitate navigating through directories and accessing system information. Here's a table with some helpful functions and their description.\n",
    "\n",
    "| Function | Description |\n",
    "| :- | :- |\n",
    "| getcwd() | Retrieve the <u>**c**</u>urrent <u>**w**</u>orking <u>**d**</u>irectory. This is the location where Python thinks you're working from. This is usually the directory holding your Jupyter notebook. | \n",
    "| chdir() | <u>**Ch**</u>ange the current working <u>**dir**</u>ectory. This can be an absolute path (ie `C:/Users/yourname/`) or relative to where you currently are (ie `data/`) |\n",
    "| listdir() | <u>**List**</u> the files and folders in the current <u>**dir**</u>ectory (default) or in the directory specified as a parameter. |\n",
    "| mkdir() | <u>**M**</u>a<u>**k**</u>e a new <u>**dir**</u>ectory (aka folder) in the current directory (default) or by providing a relative or absolute path. Note: you will get an error if attempting to make a directory that already exists. |\n",
    "| rename() | Rename a folder or file. You can even move things to new directories as long as the destination path exists. |\n",
    "| rmdir() | <u>**R**<u>e</u>**m**</u>ove a <u>**dir**</u>ectory but will give an error if the directory does not exist |\n",
    "| remove() | Remove a file (path) as long as it exists. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the os package. No need to rename it.\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is my current working directory?\n",
    "...\n",
    "\n",
    "# Use the print() command to make it slightly more readable\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change current working directory if needed\n",
    "os.chdir(...)\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in directory\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create or make a directory\n",
    "...\n",
    "\n",
    "# Check out how that's changed the directory\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename a directory or file\n",
    "...(src = \"test_directory\", dst = \"renamed_directory\") \n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...(\"renamed_directory/\") # Remove a directory \n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2.2.0 Use pandas `read_excel()` to import your data\n",
    "\n",
    "So we covered an important series of `os`-based commands that are helpful in moving and renaming our data. Sometimes the data itself might have issues, which is all part of data wrangling. To simplify our process, however, the `pandas` package has already created a function to deal with headers and other issues that can appear when importing __.xlsx__ files and yes, it runs `openpyxl` under the hood (See __Appendix 1__ in section __6.0.0__ of this lecture).\n",
    "\n",
    "By default `read_excel()` will use the __first sheet__ in the workbook but we can also use the `sheet_name` parameter with the actual sheet name, or it's sheet position. The resulting imported data will be stored in a `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file in from the data folder to a DataFrame with the first sheet as default\n",
    "...\n",
    "\n",
    "# By default, Jupyter Notebooks will only show us a truncated version of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a specific sheet in based on sheet name\n",
    "pd.read_excel(\"infection_data_all.xlsx\", sheet_name=\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.2.1 Use pandas `ExcelFile()` class to get a list of sheet names\n",
    "\n",
    "Although we can read in sheets by position, unless we know the sheet names _a priori_ it makes it hard to call on a sheet name specifically. It can be done quite easily with the `openpyxl` package (See __Appendix 1__, section __6.0.0__) but we can also intialize an `ExcelFile` object which will have the `.sheet_names` property. After obtaining the sheet names, we can also use the `ExcelFile.parse()` method on the `ExcelFile` object we create.\n",
    "\n",
    "In fact, over the years, the method `ExcelFile.parse()` has become virtually identical to the `read_excel()` function if we were to take a look under the hood. Remember that to use this method, we will also apply the dot `.` notation syntax to our `ExcelFile` object.\n",
    "\n",
    "You can also select multiple specific sheets to read in after you've created an `ExcelFile` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an ExcelFile object which will contain information about our excel file\n",
    "infection_xl = ...('infection_data_all.xlsx')\n",
    "\n",
    "# What is this object?\n",
    "type(infection_xl)\n",
    "\n",
    "# List the sheet names from the sheet_names property\n",
    "infection_xl..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take our ExcelFile object and parse a specific file\n",
    "infection_xl.parse(sheet_name = ...)\n",
    "\n",
    "# Close the file when we're done with it. This releases it from memory.\n",
    "infection_xl.close()\n",
    "\n",
    "# Here's equivalent code if you only want one sheet at a time. \n",
    "# We'll talk more about this particular syntax soon!!!\n",
    "pd.ExcelFile('infection_data_all.xlsx')..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.2.2 Read multiple sheets into a `dictionary` of `DataFrame` objects\n",
    "\n",
    "Suppose you wanted to select multiple sheets. The `read_excel()` function will parse through a list __`[ ]`__ of sheet indices (zero-indexed of course!) and names to store them as entries in a dictionary object using the `sheet_name` arguments as keys. With this parameter you can:\n",
    "\n",
    "1. Mix indices and sheet names in your list!\n",
    "2. Use `None` to open all sheets in your file\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Be careful what you ask for:</b> When using the option to identify sheets by their <i>index</i>, you must remember that these will take the place of your actual sheet name as a dictionary key.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in multiple sheets\n",
    "infection_xl_dict = pd.read_excel('infection_data_all.xlsx', sheet_name=...)\n",
    "\n",
    "# What is the object type?\n",
    "type(infection_xl_dict)\n",
    "\n",
    "# Look at the keys\n",
    "infection_xl_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infection_xl_dict[...]\n",
    "\n",
    "# What kind of data structure has sheet 3 been imported as?\n",
    "type(infection_xl_dict[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all sheets from a book\n",
    "# 'header' uses row number(s) (0-indexed) provided\n",
    "# header=0 will use the first row to determine column names\n",
    "pd.read_excel('infection_data_all.xlsx', sheet_name=..., header=0).keys() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://github.com/uoft-csb-datasci/CSBdatasci_Course_Materials/blob/main/IntroPython/taken_readr.png?raw=true\" width=700>|\n",
    "|:--:|\n",
    "|While there are many options for opening excel files, importing directly with `Pandas` is most convenient. However, other file types will require different methods.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Section 2.0.0 Comprehension Question:</b> Import the 4th, 2nd, and 3rd sheets (in this order) from the file <b>infection_data_all.xlsx</b> into a dictionary object using their <b>sheet names</b> as the key.   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprehension answer code 2.0.0\n",
    "# Import the 4th, 2nd, and 3rd sheets (in that order) from infection_data_all.xlsx\n",
    "# Use the .keys() method to check!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 3.0.0 Data wrangling with `pandas`\n",
    "\n",
    "Next, we will import the data files we discussed at the top of class and reformat them in such way that we can join the two files into a single, long-format Pandas `DataFrame`. The image below is a screenshot of the two files that make up the  dataset: one file with metadata and experiment information, and one with experimental measurements. How would you perform the joining? What could we use as a key to join the two datasets? (Think of this question as \"what do the two datasets have in common?). \n",
    "\n",
    "Importing and exploring your dataset, and designing a data wrangling strategy, is something that you should always do before the start of reshaping your dataset.\n",
    "\n",
    "Let's take a quick minute to explore the files using Excel and to propose a data wrangling strategy or even start reshaping your data. Avoid scrolling down for now as the answers are in this notebook. Also, make sure to make copies of the two files and use those for your exploration. (Data inspection is also done in Python, of course, but for the sake of this exercise, use Excel.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/uoft-csb-datasci/CSBdatasci_Course_Materials/blob/main/IntroPython/datasets_screenshot.png?raw=true\" width=1000></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit hard to see but here's where we want to be at the end. Remember our end objective will be to merge these two datasets into a single long-format dataset.\n",
    "\n",
    "<center><img src=\"https://github.com/uoft-csb-datasci/CSBdatasci_Course_Materials/blob/main/IntroPython/merged_dataset.jpg?raw=true\" width=1000></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll talk in more detail about how to get there but let's talk general strategy first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.0.1 A general strategy for data wrangling\n",
    "\n",
    "Here is a general overview on how to convert the files into long format and join them into a single data frame:\n",
    "\n",
    "1. __Import and explore the file__ to identify issues that might be present with your data. What general problems may arise as you wrangle this dataset?\n",
    "\n",
    "2. __Create a subset of your data.__ Sometimes you are working with __VERY__ large datasets. The process of wrangling a large file can take a lot of processing power and time. You don't want to spend your time experimenting on the entire dataset to come up with a customized procedure to wrangle it.\n",
    "\n",
    "3. __Remove unwanted data.__ Some columns or rows may be irrelevant to your analysis or unuseable due to missing data discovered in step 1. Eliminate these as needed.\n",
    "\n",
    "4. __Convert your dataset__ into long format to help conform with tidy data principles.\n",
    "\n",
    "5. __Remove any additional unwanted data__ such as NA values that could not be removed in step 3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.1.0 Data wrangling with Pandas\n",
    "\n",
    "Data preprocessing, a.k.a. ___data wrangling or data munging___, is the phase in which the data is prepared for downstream data analysis. It involves data cleaning, formatting, handling of special characters in datasets, etc. Preprocessing datasets can be labour intensive depending on the initial state of the data, as we will see today, but `Pandas` simplifies this process to a great extent. \n",
    ">\"Pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with __'relational'__ or __'labeled'__ data both easy and intuitive.\" \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Read more:</b> You can find the pandas package documentation <a href=\"https://pandas.pydata.org/docs/getting_started/overview.html\">here</a>.\n",
    "</div>\n",
    "\n",
    "Let's get down to it. First, import `Pandas` and `Numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# We have already imported pandas but do it again so the sections are independent\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Import our CSV file with `read_csv()`\n",
    "\n",
    "Now we are all set up to tackle file 1: `infection_meta.csv`. This is a comma-separated version of the metadata data from our measurements. This dataset tracks information for each experimental condition measured including experimental dates, reagent versions, and sample locations. We'll use this file to ease our way into importing, manipulating, and exporting in today's class.\n",
    "\n",
    "We'll import the data with the pandas `read_csv()` function which will produce a `DataFrame` object from the import. Some useful parameters for this function include:\n",
    "\n",
    "- `filepath_or_buffer`: the source or location of our data\n",
    "- `sep` or `delimiter`: a string denoting how your data are separated usually a `,` (default), `\\s` (space) or `\\t` (tab).\n",
    "- `header`: the row number(s) to use as the column names.\n",
    "- `index_col`: the column(s) to use as index (row labels) for your DataFrame.\n",
    "- `na_values`: a list of values that can be considered `NaN` _in addition to_ a comprehensive default list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick tour through the read_csv function will show there are quite a few parameters to play with\n",
    "pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# na_values=' ' makes empty cells into NaN\n",
    "embryo_metadata = pd.read_csv(...) \n",
    "\n",
    "# Take a peek at the table\n",
    "embryo_metadata.head()\n",
    "embryo_metadata.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "From a quick glance at the data output, we can see a few things about `embryo_metadata`. There are 29 columns in the data that represent various bits of metadata regarding specific experiments listed in the `experiment` column. We have 276 rows of data in the dataset.\n",
    "\n",
    "## 3.2.0 Data(Frame) inspection using methods and properties\n",
    "\n",
    "As mentioned before, inspecting your data to understand aspects such as types, missing values, and shape, is a crucial part of data preprocessing. Recall that methods and properties like `info()`, `head()`, `tail()` and `.shape`, are valuable starting points. We've already looked at the top and bottom of our data frame, so let's get some more basic data about it.\n",
    "\n",
    "- Recall that the `.info()` method provides basic information about each column including how many values are in each column. However, with too many columns, that information is truncated or summarised.\n",
    "- The `.shape` property will only provide the basic dimensions of the `DataFrame` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab some summary information on our DataFrame\n",
    "embryo_metadata...    \n",
    "\n",
    "# If a DataFrame has too many columns, only summarised information will be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What are the dimensions of our data?\n",
    "embryo_metadata...\n",
    "\n",
    "# shape is a property of data frames, reason why we don't use parenthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "From our intial inspection, we can see now there are 276 observations in our data along with the 29 columns. Recall from our `.info()` call, however, that we saw a number of columns where there were not 276 `non-null` values, but rather a smaller number like in the `Time plated` and `Time Incubated` columns. What does it mean to have `non-null` values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Missing values are replaced with `NaN`\n",
    "\n",
    "If we were to open our dataset `infection_meta.csv` outside of Python in a program like Excel, we would find that the data is not perfectly complete. There are cells without any values present or we see \"NA\" as a value when there should be something like an integer or description of some kind. In Excel, unused cells are mostly ignored for calculations, but `Pandas` can't do that when importing and needs to assign _some kind of value_ to each cell in the `DataFrame`. Also recall that our columns should individually be formed of the same data `type` or object!   \n",
    "\n",
    "Recall our import command: `embryo_metadata = pd.read_csv(\"infection_meta.csv\")`. By default, when it encounters specific set of values, the function will create the `NaN` as a replacement value. This list includes: _‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, ‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘n/a’, ‘nan’, ‘null’._\n",
    "\n",
    "However, amongst the many possible parameters, there is another named `na_values` where you can add to the default list of NA values, __just in case you have used a different identifier__ for incomplete or missing data! You can even set a dictionary of missing value characters specific to each column. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Read more:</b> Find more information about the import functions in pandas <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html?highlight=read_csv#pandas.read_csv\">here</a>\n",
    "</div>\n",
    "\n",
    "For now, let's take a closer look at the possible `NaN` values that are currently in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.2.2 Check for NaN values with the `.isna()` and `.any()` methods\n",
    "\n",
    "`NaN` values can represent any number of potential problems with your data. Although we quickly found `NaN` values in one of our rows using just a cursory inspection, there may be other `NaN` values imported that we haven't seen or expected. We can check for more `NaN` values with the `.isna()`  DataFrame method which will return a same-sized object where any `NaN` or similar values are mapped to the value `True`.\n",
    "\n",
    "To accomplish this task we'll also apply the `DataFrame.any()` method in-line with our `isna()`. Similarly there is a `.isnull()` function that you could use as well. The `any()` _function_ is a core Python function that checks if _any_ of the elements within an iterable (such as a list) have the value `True`. In the case of the DataFrame class, the `any()` method will evaluate along the specific axis. The `any()` method parameters include:\n",
    "\n",
    "- `axis`: indicates which axis should be reduced. \n",
    "    - `0` or `index` (default) reduces the index dimension and returns a Series whose index is the original column labels. In other words, apply `any()` to each column and return a single row!\n",
    "    - `1` or `columns` reduces the columns to return a Series whose index is the original index. Thus apply `any()` to each entire row.\n",
    "    - `None` reduces all dimensions to return a single boolean scalar\n",
    "    \n",
    "- `bool_only`: include only boolean columns, default is `None` (use everything)\n",
    "\n",
    "We'll also revisit the `.columns` property from last lecture to return an `Index` object containing all of the column names.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Do I use <i>isna()</i> or <i>isnull</i>()?</b> What is the difference between these two functions? Nothing. You'll find the documentation is identical as Pandas really only has <b>NaN</b> values. However, historically the ideas in Pandas are built upon the Dataframe object from the programming language <b>R</b> which distinguishes between NA and NULL values.\n",
    "</div>\n",
    "\n",
    "\n",
    "Now that we're familiar with the methods, we can ask a question like: \"How many columns contain `NaN`?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call isna() on just the DataFrame\n",
    "embryo_metadata..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Look, we could potentially see which cells have `NaN` values, except most of the output is truncated for us. We need a better way on large data frames - like with the `any()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the any() method to the same call\n",
    "embryo_metadata.isna()...\n",
    "\n",
    "# What kind of object is returned from our call?\n",
    "type(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.2.3 Use a boolean list to retrieve columns with `NaN` values.\n",
    "\n",
    "Our above call created a `Series` object with essentially one entry for every column in our DataFrame. If our DataFrame had many more columns, we would only get a summary of the first and last 5 columns. How would we know which columns have `NaN` values? It can be a metaphorical needle in a haystack! You need to narrow down your search by thinking smarter not working harder. \n",
    "\n",
    "Recall from __lecture 02__ that you can supply a list of booleans to certain data structures (like `Numpy.arrays`) in order to retrieve their elements. Only `True` values return the specific element at that index. In this case we'll index/filter our column names based on their `.isna().any()` results!\n",
    "\n",
    "We'll use the `.columns` property to help us figure out which columns have `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull up a list of columns from our DataFrame\n",
    "embryo_metadata...\n",
    "\n",
    "type(embryo_metadata...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take our series of booleans and retrieve ONLY the column names that are true\n",
    "embryo_metadata.columns[embryo_metadata...]\n",
    "\n",
    "# What is the object we get back?\n",
    "type(...)\n",
    "\n",
    "# How long is our list?\n",
    "len(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Okay, so it looks like only 3 columns have `NaN` values. What can we do with this information?\n",
    "\n",
    "### 3.2.4 Use the `.tolist()` method to convert your objects\n",
    "\n",
    "From above we see that all of our code creates an `Index` object which is part of the `pandas` package. It has some extra properties and methods that might be useful but we can also simply convert it to a simpler Python `list` if all we care about is the column names. To accomplish this we'll use the `pandas` method `.tolist()` which will iterate through the object and produce a simple list of python scalars or pandas scalars for more complex values like Timestamps or intervals.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Food for thought:</b> We could also use the <b>list()</b> function BUT things can get more complicated when working with nested lists or nested structures. The <b>list()</b> function was built as part of Python and doesn't necessarily have detailed behaviours for handling optional packages like pandas. Therefore, it is best to use the methods and functions provided (when available) by a package to coerce or re-cast objects.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pprint\n",
    "# Use the tolist() method\n",
    "embryo_metadata.columns[embryo_metadata.isna().any()]...\n",
    "\n",
    "# Cast the information to a simple list\n",
    "# list(embryo_data.columns[embryo_data.isna().any()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.2.5 `.loc[]` and `.iloc[]` can accept boolean lists as well\n",
    "\n",
    "So we have a way to get a list of columns with `NaN` values. In fact we can use similar methods to retrieve column names for any columns that meet conditional criteria we are interested in. Suppose we wanted to cull a DataFrame down to the information we are explicitly interested in? \n",
    "\n",
    "While we discussed the uses of `.loc[]` and `.iloc[]` in the context of having column names and index positions, both of these methods can accept `boolean` series/arrays/lists as a means to filter for specific columns. Remember that multi-indexing uses a `[row, column]` format so how do we get just our columns with `NaN` data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `.loc` to retrieve a slice of our DataFrame with columns containing NaN values\n",
    "embryo_metadata...[:, embryo_metadata.isna().any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.2.6 Additional value-checking mechanisms\n",
    "\n",
    "Both the `.isna()` and `.isnull()` methods also have counterparts that perform the opposite behaviour: `.notna()` and `.notnull()` which essentially flip the boolean values from `.isna()` and `.isnull()` respectively. \n",
    "\n",
    "Let's look at the output from `.isnull()` first and then check out `.notna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also returns booleans, like isna()\n",
    "embryo_metadata...\n",
    "\n",
    "# How many columns have null values?\n",
    "len(embryo_metadata.columns[embryo_metadata.isnull().any()])\n",
    "\n",
    "# Remember these are booleans so we can sum them!\n",
    "embryo_metadata.isnull().any()...\n",
    "\n",
    "# Which ones?\n",
    "embryo_metadata.columns[embryo_metadata.isnull().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many columns contain non-NaN values?\n",
    "embryo_metadata...\n",
    "\n",
    "# Note that this is not the strict opposite result of what we expected! Why is that?\n",
    "# The functions do have opposite behaviours though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is 29 the answer we expect to see from above code? Are we asking which columns have no NA values or simply which columns contain values that are not NA? There is a fine distinction in this logical statement and we know that all of our columns (29) have _some_ data in them! \n",
    "\n",
    "***\n",
    "### 3.2.7 Use method chaining to reduce intermediate variables\n",
    "\n",
    "What have we been doing in the past handful of coding cells? While we've been encountering some similar examples in this section, we haven't really addressed this coding style. One of the great things in working with an object oriented language like Python is that we can use methods in a tandem order, also known as method chaining. Many methods or properties return an object and ___as long as we know what kind of object is returned___, we can chain method calls on the resulting object. This is especially helpful in the `Pandas` package where core functions are often present as class methods.\n",
    "\n",
    "From above:\n",
    "1. `embryo_data.notna()` returns a `DataFrame` upon which we call...\n",
    "2. `DataFrame.any()` which returns a `Series` upon which we call...\n",
    "3. `Series.sum()` which returns a scalar in this case\n",
    "\n",
    "Of course you need to know which objects you are dealing with and their methods but it makes our code look simpler overall!\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Reduce intermediate variables!</b> By using method chaining, we not only simplify our code but also reduce intermediate variables. If we had used simpler programming methods, we would need to produce a temporary variable to hold the output in each step outlined above before getting a final answer. This may not be a big problem in some cases but when we have no need for the intermediate results, method chaining keeps the process clean in our code, and in program memory. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.3.0 Subsetting your `DataFrame`\n",
    "\n",
    "Now that we've explored our metadata file a little bit and know that there are some missing values in the `Time plated`, `Time Incubated` and `Location` columns, we can proceed to looking at a more complex set of data using `embryo_data_wide.csv`. Let's begin by importing and taking a cursory look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import embryo_data_wide.csv\n",
    "embryo_wide = pd.read_csv(...)\n",
    "\n",
    "embryo_wide.head()\n",
    "embryo_wide.tail()\n",
    "embryo_wide.info()\n",
    "\n",
    "# How many columns with NaN values?\n",
    "embryo_wide.isna().any().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "As we can see from our initial glance, there are 154 observations and 901 columns (variables). To break it down, our data takes the form of:</br></br>\n",
    "\n",
    "|worm.number|`<date>_<wormStrain>_<pathogenStrain>_<pathogenDose>_<timepoint>_<measurementType>`|...|\n",
    "|:-: |:-: |:- |\n",
    "|A worm number for observation|`sporesPresent` __OR__ `merontsPresent` __OR__ `numEmbryos`|...|\n",
    "\n",
    "</br></br>\n",
    "So we have a few things to address:\n",
    "1. The data is definitely in a wide format where we have _multiple_ observations per row/`worm.number`\n",
    "2. Within each column ___name___ we have stored __6__ pieces of data alone\n",
    "3. Within each column ___value___ we have stored __1__ piece of information belonging to one of __3__ measurement types: `sporesPresent`, `merontsPresent`, or `numEmbryos`. __Therefore, every 3 columns belong together in the same experiment!__\n",
    "4. NA values are present in 897/901 columns (each experiment had a variable number of animals measured).\n",
    "\n",
    "However, before proceeding, we need to recall this is a pretty wide dataset. It would be better for us to figure out our code on a smaller piece that we'll call our _subset_ of data.\n",
    "\n",
    "***\n",
    "### 3.3.1 Before reshaping `embryo_wide` take a subset\n",
    "\n",
    "A recap on the formating to be done on the metadata file:\n",
    "\n",
    "1. Import and explore the file to identify issues that might be present with your data. What general problems may arise as you wrangle this dataset? $\\checkmark\\checkmark$\n",
    "\n",
    "2. __Create a subset of your data. Sometimes you are working with VERY large datasets. The process of wrangling a large file can take a lot of processing power and time. You don't want to spend your time experimenting on the entire dataset to come up with a customized procedure to wrangle it.__\n",
    "\n",
    "3. Remove unwanted data. Some columns or rows may be irrelevant to your analysis or unuseable due to missing data discovered in step 1. Eliminate these as needed.\n",
    "\n",
    "4. Convert your dataset into long format to help conform with tidy data principles.\n",
    "5. Remove any additional unwanted data such as NA values that could not be removed in step 3!\n",
    "\n",
    "As mentioned previously, preprocessing data with large datasets is a rather inefficient strategy - not so bad if you have access to vast computational power but still inefficient. There is quite a bit of trial and error involved to find out what works best on your data, and the larger the dataset, the longer it will take the output to be generated for you to inspect and evaluate it. \n",
    "\n",
    "Instead, we take a representative sample of the full dataset, write code to preprocess it, and then apply the program we wrote to the full dataset. For example, the full `embryo_wide` DataFrame has 154 rows and 901 columns, where column indices 1 and onwards (zero-indexation) are experimental entries. Nine of these columns will be enough for data wrangling so we can ignore 891 columns for now. The easiest way to subset data would be using `.iloc[]` or `.loc[]` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the first 100 rows of data and 10 columns\n",
    "embryo_wide_subset = embryo_wide.iloc[...]\n",
    "\n",
    "# View our subset\n",
    "embryo_wide_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.3.2 Subset your data randomly with Numpy's `random.choice()` function\n",
    "\n",
    "Sure we could take a simple subset of our data but we can also take a random subset selection that is representative of our data set. Taking random subsets is especially helpful if you are running certain tests on data for machine-learning or programming/testing new algorithms.\n",
    "\n",
    "To accomplish our subsetting goals, we will use Numpy's `random.choice()` function to generate a representative subset of `embryo_wide`. Of course we'll still want the first column - it's the experimental measurements that we want to randomize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "np.random.choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Random seed (pseudo-random number generator):</b> A function used to ensure reproducibility when a program involves randomness. It is used in processes such as random number generation or sampling from a dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know more about **random.choice()**, let's use it. Given that the ***a*** argument expects a 1-D array __OR__ an integer (see ***random.choice***'s documentation), we will use the ***.shape*** property to retrieve integer values of the highest number of rows and columns in our data.\n",
    "\n",
    "Note that when given an integer __n__ for the `a` parameter, a range of integer values is created from 0 to __n-1__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed = 2026)     # The number to which you \"set the seed\" must be the same in order to obtain the same results (reproducibility)\n",
    "\n",
    "# Let's start by sampling 100 rows (shape[0])\n",
    "np.random.choice(a  = embryo_wide..., size = 100, replace = False)\n",
    "\n",
    "# Now sample 10 columns (shape[1])\n",
    "np.random.choice(a  = embryo_wide..., size = 10, replace = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Done. Subsampling from both rows and columns seems to be working as expected so now we can create a subset. How do we use those two lines of code to sample from `embryo_wide` without ending up with booleans for a final ouput?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our random seed\n",
    "np.random.seed(seed = 2026)\n",
    "\n",
    "# We use squared brackets to subset embryo_wide and get actual values instead of booleans\n",
    "embryo_wide_subset = embryo_wide.iloc[np.random.choice(a = embryo_wide.shape[0], size = 100, replace = False), # subset rows\n",
    "                                       np.random.choice(a = embryo_wide.shape[1], size = 10, replace = False)]   #subset cols\n",
    "\n",
    "embryo_wide_subset..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.3.3 Make a range of numbers with `np.arange()`\n",
    "\n",
    "While the subset looks fairly good, it is not quite what we wanted. We did not catch the _first_ column and we __need__ it. On a side note, seeing only experimental columns is simply a matter of probability as there are hundreds of experimental columns versus 1 worm number column. Remember we are merely giving `.iloc[]` a list of index positions. All we have to do is provide a static and random portion and put them together. \n",
    "\n",
    "To achieve this we'll use the Numpy `arange()` function. No that's not a typo. Python 3 already has `range()` which returns a generator function based on the range specified. Numpy's version returns an ***a***rray for the ***range*** supplied from start (inclusive) to end (exclusive). \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>What is a generator?</b> While we will cover things in more detail our flow control lecture (Lecture 05), you should know that generators are functions which produce a group of values that can be run/looked through when supplied to the correct kind of Python functions.\n",
    "</div>\n",
    "\n",
    "To put two arrays together, we use the `np.append()` function. Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make two ranges and append them together\n",
    "np.append(np.arange(0, 5), ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our seed for consistency\n",
    "np.random.seed(seed = 2026)  \n",
    "\n",
    "# Make our multi-index call\n",
    "embryo_wide_subset = embryo_wide.iloc[np.random.choice(a = embryo_wide.shape[0], size=100, replace = False), # Random rows\n",
    "                                np.append(0, # Grab the first column\n",
    "                                          # Randomly select 9 more columns after that\n",
    "                                          np.random.choice(a = np.arange(...), \n",
    "                                                           size = 9, replace = False)\n",
    "                                         ) # End our append call\n",
    "                               ] # Multi-index call\n",
    "\n",
    "# Check it's dimensions\n",
    "embryo_wide_subset.shape\n",
    "\n",
    "embryo_wide_subset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice any relationship between our index labels and the `worm.number` column?\n",
    "***\n",
    "### 3.3.4 Use the DataFrame `sample()` method as an alternative to `np.random.choice()`\n",
    "\n",
    "An alternative to `random.choice()` for random subsampling of either rows or columns is the `sample()` method. Using the parameters you can decide on:\n",
    "\n",
    "- `n`: the number of items from `axis` to return.\n",
    "- `frac`: an alternative to a number of items, take a fraction of the total items. Cannot be used with `n`.\n",
    "- `axis`: either `0` for rows (default for Series and DataFrames), `1` for columns.\n",
    "- `replace`: allow or disallow sampling of the same item more than once.\n",
    "- `random_state`: the parameter for setting a random seed for the pseudorandom generator. Otherwise it sets its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Jupyter help window instead of printing the information to an output cell\n",
    "embryo_wide_subset.sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull 100 rows from our original DataFrame object\n",
    "embryo_wide.sample(n = 100, replace=False, weights=None, ...).head()\n",
    "\n",
    "# weights is all rows have equal probability of being picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also subset by a fraction of the dataset\n",
    "embryo_wide.sample(..., replace = False, weights=None, random_state=1, axis=0) \n",
    "\n",
    "# 3.45% of rows (axis 0) and all columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Because of the nature of our dataset, we'll go back with our ___simple version___ of subsetting in order to maintain our data in __groups of 3__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the first 100 rows of data and 10 columns\n",
    "embryo_wide_subset = embryo_wide.iloc[0:100, 0:10]\n",
    "\n",
    "# Make a quick copy of our subset\n",
    "# Remember this is just a shallow copy BUT all of our data is in that first level anyways\n",
    "embryo_wide_copy = embryo_wide_subset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.4.0 Reshaping your data\n",
    "\n",
    "So we've discussed a few ways to generate subsets from our data using `numpy.random.choice()` and `pandas.DataFrame.sample()`. This is helpful when generating code to reshape our data because the original information can be extremely dense and, in some cases, computational intensive to just \"play\" with. We've even peeked at our subsets as well, just to help us sanity check for larger issues we may have overlooked when initially importing the data. \n",
    "\n",
    "Let's see where we are in our steps:\n",
    "\n",
    "1. Import and explore the file to identify issues that might be present with your data. What general problems may arise as you wrangle this dataset? $\\checkmark\\checkmark$</br></br>\n",
    "\n",
    "2. Create a subset of your data. Sometimes you are working with VERY large datasets. The process of wrangling a large file can take a lot of processing power and time. You don't want to spend your time experimenting on the entire dataset to come up with a customized procedure to wrangle it. $\\checkmark\\checkmark$</br></br>\n",
    "\n",
    "3. __Remove unwanted data. Some columns or rows may be irrelevant to your analysis or unusable due to missing data discovered in step 1. Eliminate these as needed.__\n",
    "    - In the current case we do see some `NaN` values in each column. However, we are going to let those remain for now and deal with them ___after___ we have reshaped our data. If we were to try and remove them at this point, we would need to remove _entire rows_ which might otherwise contain bits of data. If, however, there was an extraneous column we could choose to remove it (like an extra `worm.number` column).</br></br>\n",
    "    \n",
    "4. Convert your dataset into long format to help conform with tidy data principles.\n",
    "5. Remove any additional unwanted data such as NA values that could not be removed in step 3!\n",
    "\n",
    "Up until this point we haven't really _altered_ any of the data we imported. As per our data wrangling plan, however, let's pretend we need to remove the `worm.number` column from `embryo_wide`. \n",
    "\n",
    "Before we start deleting anything from the dataset, inspect the dataset and pull out a list with column names. The fastest way to retrieve a list of column names is to cast the DataFrame as a `list()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 rows\n",
    "embryo_wide_subset.head()\n",
    "\n",
    "# Cast the DataFrame to a list \n",
    "# We should have just 10 columns as a result\n",
    "list(embryo_wide_subset)\n",
    "\n",
    "# Alternative code but it's longer\n",
    "# Just print the first 5 columns to verify the process works\n",
    "embryo_wide_subset...[0:5].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.4.1 Remove a column using the `.drop()` method\n",
    "\n",
    "As usual, there are several ways to achieve the same goal. In this case, we want to remove the first column, `worm.number`, from the `embryo_wide_subset` object. We can remove that column by its index using the `.drop()` method from the `DataFrame` class. This is accomplished in a single call or _one-liner_ (nickname for programs that are one line long).\n",
    "\n",
    "You can complete this code using the index position(s) or the column name(s). The parameters we want to remember are:\n",
    "\n",
    "- `label`: the index __or__ column labels to drop.\n",
    "- `axis`: the axis you wish to drop from (0 = index/rows, 1 = columns).\n",
    "- `inplace`: make the change in place on the calling DataFrame (`True`). This will silently update the original `DataFrame`. Otherwise (`False`, the default) a new copy of the `DataFrame` is returned.\n",
    "- `index` \\ `columns`: use this parameter to directly supply a list of columns to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a column by its index\n",
    "embryo_wide_subset...(labels = embryo_wide_subset.columns[[0]], \n",
    "                        axis = 1,\n",
    "                        inplace = True)\n",
    "# this syntax will remove one every time you run the code\n",
    "\n",
    "# Look at the resulting DataFrame\n",
    "embryo_wide_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Removing by index can be dangerous!</b> Even though the above code does its job as expected, <i>it can be very dangerous</i>. Since it uses index number/position to locate the column we wish to remove, once the current column 0 is removed, all other columns will be shifted one position to the left. As a consequence you will now have another column at index 0, and if you run this piece of code again, it will continue removing whatever column is present at index 0. You can end up removing many more columns than you intended, by mistake, with potential serious negative implications for your analyses. \n",
    "\n",
    "Thus, it is <i>safer</i> to use a syntax that explicitly looks for the column of interest, like using the full column name or by partial matching (more on that in <b>Lecture 6</b>) or by setting the label value to a variable first before using the __drop()__ method.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore our subset since we've already altered it\n",
    "embryo_wide_subset = embryo_wide_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove a column by its name/label\n",
    "embryo_wide_subset.drop(..., inplace=True) # labels are indices names for either columns or rows. \n",
    "# 'inplace=True' overwrites dataframe instead of us explicitly re-assigning the variable\n",
    "\n",
    "# del my_dataframe['column_name'] # This is another way to remove the column by its name\n",
    "\n",
    "# Look at the resulting dataframe\n",
    "embryo_wide_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "If we try to run this same code again, we get a KeyError because there is no column called \"worm.number\" anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embryo_wide_subset.drop(columns='worm.number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.5.0 Melting your variable columns into just two columns\n",
    "\n",
    "Now that we've learned _how_ to remove a column from our DataFrame, let's see where we are in our steps:\n",
    "\n",
    "1. Import and explore the file to identify issues that might be present with your data. What general problems may arise as you wrangle this dataset? $\\checkmark\\checkmark$\n",
    "\n",
    "2. Create a subset of your data. Sometimes you are working with VERY large datasets. The process of wrangling a large file can take a lot of processing power and time. You don't want to spend your time experimenting on the entire dataset to come up with a customized procedure to wrangle it. $\\checkmark\\checkmark$\n",
    "\n",
    "3. Remove unwanted data. Some columns or rows may be irrelevant to your analysis or unuseable due to missing data discovered in step 1. Eliminate these as needed. $\\checkmark\\checkmark$\n",
    "    - No real columns need to be removed but we can use the `.drop()` method if required.\n",
    "\n",
    "4. __Convert your dataset into long format to help conform with tidy data principles.__\n",
    "    - Our goal is to combine all of the experiment names into a single column and to put all of their values into a corresponding column. Currently they are spread across multiple columns as column names with experimental observations in each experimental column.\n",
    "    - We'll convert our data through a process known as \"melting\" or \"pivoting\" in other programming languages.\n",
    "    \n",
    "5. Remove any additional unwanted data such as NA values that could not be removed in step 3!\n",
    "\n",
    "***\n",
    "### 3.5.1 Melting your variable columns with the `melt()` _function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://github.com/uoft-csb-datasci/CSBdatasci_Course_Materials/blob/main/IntroPython/reshaping_melt.png?raw=true\" width=700>|\n",
    "|:-:|\n",
    "|We convert our wide-format data to long-format using a function like `melt()` which compresses variable (ie column) names into a single `variable` column, and their associated values into a single `value` column. In the above example, we are compressing the two variables `height` and `weight` (blue and green) into a single column name `variable` and each of their matching measurements appears in the `value` column. Both the `height` and `weight` variables are tied to the unique identifiers defined in `melt()` by the two variables `first` (orange) and `last` (purple). This image was provided by the [Pandas online documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/).|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \"melt\" all of the descriptive __Experiment__ variable names into a single column. This essentially means we are going to compress all of the Experiment column information into _two_ columns. The first column holds the set of column names repeated over and over - once per unique `worm.number` in the data. In this case the experimental column names of:\n",
    "\n",
    "- `<date>_<wormStrain>_<pathogenStrain>_<dose>_<timepoint>_<measurementType>`\n",
    "    \n",
    "while the second column holds the `value` which is dependent upon the `<measurementType>` listed in the column name. The possible types are:\n",
    "- `sporesPresent`: boolean-type values represented as 0 or 1\n",
    "- `merontsPresent`: boolean-type values represented as 0 or 1\n",
    "- `numEmbryos`: integer values representing the number of embryos identified in a single animal.\n",
    "    \n",
    "Thus a new set of observations (rows) is added for each variable (column) we are melting. \n",
    "\n",
    "Another important aspect of this step is that the unmelted information (untouched columns) in each observation must combine to uniquely identify an observation. In our case, __column 0__ (`worm.number`) holds our unique identifier information and will not be altered by `.melt()`. That data column, however, will now be repeated multiple times for each new variable/column (experimental code) that is converted to an observation.\n",
    "\n",
    "We'll accomplish this with the Pandas function `melt()` or the identical DataFrame method of the same name. The key parameters to keep in mind are:\n",
    "\n",
    "- `frame`: the DataFrame we want to melt if using `pd.melt()`.\n",
    "- `id_vars`: a tuple/list/array of column name(s) to use as identifiers. These are the _unique columns_ in our data that differentiate between observations. They will be \"unchanged\" by the melt. Note that these must be a list of __column names/identifiers__ and NOT indices.\n",
    "- `value_vars`:  a tuple/list/array of column name(s) to use as variable columns. If `None`, then all __non-__`id_vars` columns will be used as variable columns.\n",
    "- `var_name`: the name for your variable column, defaults to `variable`.\n",
    "- `value_name`: the name for your variable values column, defaults to `value`.\n",
    "\n",
    "Note that regardless of using the `pd.melt()` or `DataFrame.melt()` versions, they will both return a __NEW__ `DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore our copy of the subset\n",
    "embryo_wide_subset = embryo_wide_copy.copy()\n",
    "\n",
    "# Remind ourselves what the DataFrame looks like\n",
    "embryo_wide_subset.head()\n",
    "\n",
    "# How big is our subset again?\n",
    "embryo_wide_subset.shape\n",
    "\n",
    "# Take a quick look at the column names again\n",
    "embryo_wide_subset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call on the melt function\n",
    "pd.melt(frame = embryo_wide_subset,\n",
    "        id_vars= ...,  # We set our unique identifier to the worm.number\n",
    "        var_name=...,                   # All other column names go here\n",
    "        value_name=...)               # Values from the columns melted go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Note the size change in our `DataFrame`? What once was 100 rows x 10 columns has now become 900 rows x 3 columns. 9 columns of 100 values became 2 columns of 900 values!\n",
    "\n",
    "### 3.5.2 Use the `DataFrame` object `.melt()` method directly\n",
    "\n",
    "While the pandas package has it's own `melt()` function, you can directly call on the `.melt()` method when working directly with a `DataFrame`. Both have their advantages since the `melt()` function can be used when cycling through a list of DataFrames for instance. Using the `.melt()` method directly reduces potential typo issues that could accidentally access the wrong DataFrame variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call on the melt method from the DataFrame object itself\n",
    "embryo_wide_subset.melt(id_vars= embryo_wide_subset.columns[0],\n",
    "                        var_name=\"experiment\",\n",
    "                        value_name=\"measurements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.5.3 Provide a list of column names as unique IDs to the `melt()` function\n",
    "\n",
    "If you wanted to keep a subset of identifier columns that were not strictly located adjacent to each other in the DataFrame, you have a few options for creating that list to pass on to the `id_vars` parameter of `melt()`:\n",
    "\n",
    "1. Make an explicit list of column names\n",
    "2. Slice and combine subsets of columns from your DataFrame to make a secondary `DataFrame`! Just remember any excluded columns will become variable columns unless you explicitly name those as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of variables\n",
    "my_id_variables = ...\n",
    " \n",
    "# Melt the subset\n",
    "pd.melt(embryo_wide_subset,\n",
    "        id_vars= my_id_variables,\n",
    "        var_name= 'experiment',\n",
    "        value_name= 'measurements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You may also pass the whole DataFrame to id_vars\n",
    "# Remember to pass a LIST to index your DataFrame and get a DataFrame back!\n",
    "my_id_variables = embryo_wide_subset.loc[...]\n",
    " \n",
    "# Melt the subset\n",
    "# hmp_data_subset_melt_bad = \n",
    "pd.melt(embryo_wide_subset,\n",
    "        id_vars= my_id_variables,\n",
    "        var_name= 'experiment',\n",
    "        value_name= 'measurements')[\"experiment\"].unique() # Check for unique values in the \"experiment\" column\n",
    "\n",
    "# Note the output will just be an array of unique experiment names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.6.0 Remove NA values in your long-format data\n",
    "\n",
    "With our data now in a long format, we can head to our last step(s):\n",
    "\n",
    "1. Import and explore the file to identify issues that might be present with your data. What general problems may arise as you wrangle this dataset? $\\checkmark\\checkmark$\n",
    "\n",
    "2. Create a subset of your data. Sometimes you are working with VERY large datasets. The process of wrangling a large file can take a lot of processing power and time. You don't want to spend your time experimenting on the entire dataset to come up with a customized procedure to wrangle it. $\\checkmark\\checkmark$\n",
    "\n",
    "3. Remove unwanted data. Some columns or rows may be irrelevant to your analysis or unuseable due to missing data discovered in step 1. Eliminate these as needed. $\\checkmark\\checkmark$\n",
    "    - No real columns need to be removed but we can use the `.drop()` method if required.\n",
    "\n",
    "4. Convert your dataset into long format to help conform with tidy data principles.\n",
    "    \n",
    "5. __Remove any additional unwanted data such as NA values__ that could not be removed in step 3!\n",
    "    - Removing some NA values prematurely may cause the loss of entire sets of observations. Now that the data is in long format, that should be simplified.\n",
    "    - You may also have cause to remove columns of extraneous data!\n",
    "\n",
    "***\n",
    "\n",
    "Our data is now mostly in the long format and from a cursory examination we can see that the `NaN` values noted previously are now located somewhere in a single column: `measurements`. This data was hard to remove before _because_ our data was in wide format, and they were scattered between columns. Removing them __by rows__ was not feasible since some rows were not composed entirely of only `NaN` observations. \n",
    "\n",
    "Now that each row itself is a single observation from a single experiment, it's easy! We can use the `DataFrame.dropna()` method to accomplish the task and get rid of our empty measurements. The `.dropna()` method has the following relevant parameters: \n",
    "\n",
    "- `axis`: determine if you want to remove on `0` (or `index`) vs. `1` (or `columns`). The default is `0`.\n",
    "- `how`: do you want to remove the segment if there are `any` vs `all` `NaN` values present.\n",
    "- `inplace`: change the DataFrame directly (`True`), or create a new one (`False`, default).\n",
    "</br></br>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Clean your method chaining:</b> As we keep building up our method chain, we have a few choices: </br>\n",
    "1) make a really long single command that will span a single line. It's hard to read and not ideal to work with.</br></br>\n",
    "2) make some intermediate variables to simplify how our code looks from section to section. This will take up some extra memory - or a lot depending on your dataset size!</br></br>\n",
    "3) Modify our dataframe <b>inplace</b> in memory along multiple steps. That's good but you could make a permanent mistake and your can't really test your code efficiently. So #2 seems like a better option. </br></br>\n",
    "4) Break open parenthesis by putting the enclosing <b>\")\"</b> on the next line.</br></br>\n",
    "5) Use parentheses <b>\"( )\"</b> to encapsulate your code. This will allow it to stretch over multiple lines.</br></br>\n",
    "6) Clean up your code with the <b>\"\\\"</b> notation which will allow us to put methods onto the next line of code BUT you can't comment after the \"\\\" or even on the next line.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our trimmed dataset into a new variable\n",
    "# Encapsulate our assignments in a set of () to stretch over multiple lines\n",
    "embryo_long_subset_trimmed = ( \n",
    "    embryo_wide_subset.melt(id_vars= embryo_wide_subset.columns[0],\n",
    "                            var_name=\"experiment\",\n",
    "                            value_name=\"measurements\"                            \n",
    "                           )\n",
    "    # Drop the NA values\n",
    "    ...\n",
    ") # Close the chain\n",
    "\n",
    "# View our trimmed subset\n",
    "embryo_long_subset_trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Half of our subsampled data was NA values! Goodbye `NaN` values!\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Why did your data have so many NaN values?</b> While it won't always be the case that your data is so riddled with missing values, our original dataset would be considered a \"ragged\" dataframe. Since each row represented an individual worm, and each column represented a single experimental condition, not each condition would necessarily measure the same number of animals. If, for example, our control set was measured much more often than the experimental conditions, this would create many rows of data with values in just the control column! The remaining columns would have NaN values that would survive our <b>melt()</b> step, leading to a large number of NaN values in the \"long-format\" version!\n",
    "</div>\n",
    "\n",
    "## 3.7.0 Correcting variables that represent multiple variables\n",
    "\n",
    "Now that we have our data into a long format, we have to deal with the issue of our `experiment` and `measurements` columns holding _multiple_ pieces of data/variables. Luckily for us it is all underscore-separated (`_`) and we can use that to our advantage. \n",
    "\n",
    "In the former case, `experiment` holds 6 pieces of metadata about each experiment regarding its \n",
    "- `date`: the date the samples were collected\n",
    "- `wormStrain`: the worm strain that was infected\n",
    "- `pathogenStrain`: the pathogen strain used to infect\n",
    "- `pathogenDose`: the dose of the pathogen in millions (M) of spores\n",
    "- `timepoint`: the hours post-infection (hpi) before the worms were fixed and collected\n",
    "- `measurementType`: this last choice could be one of 3 possible labels: `sporesPresent`, `merontsPresent`, and `numEmbryos`.\n",
    "\n",
    "In the `measurements` column, we have 3 kinds of observations: \n",
    "- `sporesPresent`: a boolean value of 0/1\n",
    "- `merontsPresent`: also a boolean value of 0/1\n",
    "- `numEmbryos`: the number of embryos per measured animal\n",
    "\n",
    "### 3.7.1 Splitting a column into multiple columns with `Series.str.split()`\n",
    "\n",
    "Since our variables are underscore-separated, we can use the `Series.str.split()` method to grab our data. This will allow us to split a string on a specific character to create a new `Series` or `DataFrame` with one column for each split. We briefly saw a similar method with base Python `str`ing objects in lecture 02. The relevant parameters here are:\n",
    "\n",
    "- `pat`: the string pattern you want to use to split your `Series`\n",
    "- `expand`: splits the strings into separate columns if `True` (returns a DataFrame). Returns a `Series` if `False`\n",
    "- `regex`: if `True` will treat `pat` as a regular expression (__Lecture 6!__) when splitting data.\n",
    "\n",
    "In order to split our data, we'll need to select a specific column to extract from. Note that we'll try this with the `.pop()` method which will _remove_ and _return_ the `experiment` column to us. This will permanently modify `embryo_long_subset_trimmed` so we don't have to remove it later!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all these steps into a variable\n",
    "experiment_Info = (embryo_long_subset_trimmed                   \n",
    "                   ...('experiment') # Recall that pop() removes and returns what was taken out (a series in this case)\n",
    "                   .str.split(pat = ..., expand = ...) # We want a DataFrame as our result\n",
    ")\n",
    "\n",
    "# Check out the resulting DataFrame\n",
    "experiment_Info.head()\n",
    "\n",
    "# What does our original one look like now?\n",
    "embryo_long_subset_trimmed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "We've split our `experiment` column but __how do we recombine it with the rest of our data?__ Before we get into that, let's fix the column names!\n",
    "\n",
    "### 3.7.2 Use the `.columns` property to update column names in your data frame.\n",
    "\n",
    "Recall that renaming our columns in our data frame can take two approaches:\n",
    "\n",
    "1. Using the `.columns` property to replace the column names in one or more positions\n",
    "2. Using the `.rename()` method to replace specific columns based on a supplied dictionary `{}` that maps old names to new ones. \n",
    "\n",
    "Let's start with the former case. Recall the `.columns` property allows us to __retrieve__ the names of the columns within a DataFrame object. However, we can also ___set___ the values of a column's name using the same property just by using the assignment operator. \n",
    "\n",
    "This can be done in subsets or all at once as long as you know the _indices_ of the columns you want to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of our experiment_Info first!!\n",
    "experiment_Info_copy = experiment_Info.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the column names\n",
    "experiment_Info... = ['date', 'wormStrain', 'pathogenStrain', 'pathogenDose', 'timepoint', 'measurementType']\n",
    "\n",
    "#Check out the result\n",
    "experiment_Info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.7.3 Use the `.rename()` method to update column names in your data frame.\n",
    "\n",
    "What happens if you don't necessarily know the position of your columns in your DataFrame or you don't want to spend the extra time looking it up with the `.columns` property. You may have an _extremely wide_ dataframe and there are better methods than looking up the information - hence the `.rename()` method.\n",
    "\n",
    "We haven't used the `.rename()` method yet but we'll use it to take the place of directly working with the `.columns` property. Renaming columns from our DataFrame is pretty straightforward but does require some thought. First, renaming a single column could normally be a simple case but what happens if we want to rename multiple columns? The Pandas package lets us rename multiple DataFrame indices (rows) OR columns with the `.rename()` method but we should take careful note of the parameters:\n",
    "\n",
    "- `mapper`: This is a `dictionary`-like object or function to apply to the axis values. This helps to link the columns names we want to change with their new values.\n",
    "- `axis`: 0 for index or 1 for columns\n",
    "- `columns`: alternative to using the `mapper` and `axis` parameters to explicitly rename just columns. We can supply a dictionary with `key:value` pairs where `key` is the current name and `value` is the updated name.\n",
    "- `inplace`: a boolean which will either directly change the DataFrame and return nothing (`True`) or return an updated DataFrame object (`False`).\n",
    "\n",
    "In our case, we'll use the `columns` parameter to denote directly what we want to change by giving it a dictionary with the `key:value` pairs mapped as `oldColumnName:newColumnName`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve our \n",
    "experiment_Info = experiment_Info_copy.copy()\n",
    "experiment_Info.columns\n",
    "\n",
    "# Change the column names\n",
    "experiment_Info...(columns = {0:'date', 1:'wormStrain', 2:'pathogenStrain', \n",
    "                                  3:'pathogenDose', 4:'timepoint', ...},\n",
    "                       inplace = True\n",
    "                      )\n",
    "\n",
    "#Check out the result\n",
    "experiment_Info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.7.4 Combine DataFrames using the `.join()` method\n",
    "\n",
    "So now we have __two sets of DataFrames__ and if you look carefully, they both share the same index labels along each observation/row. This is a good thing! That means we can easily ___match___ the columns and rows in one frame to another. Using the `.join()` method on a `DataFrame`, we can pass along a second `DataFrame` that will combine them based on either the `index` by default or a key/variable of your choosing. Note that this is a completely different method from the string.join() method we've used in previous lectures.\n",
    "\n",
    "We'll look at more complex joins soon BUT for now, know that we have a one-to-one correspondence of indices between our measurements and our metadata `DataFrame` objects! Joining our data will align the correct metadata to each observation (row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embryo_long_subset_joined = embryo_long_subset_trimmed...\n",
    "\n",
    "embryo_long_subset_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.7.5 `pivot()` to redistribute mixed column values into individual columns\n",
    "\n",
    "Recalling the details of our `measurementType` column there are __3 types of measurements__ in this single column. In this case it's a clear distinction between the different types of values in a variable like `wormStrain`. In order to conform to tidy data principles we must redistribute the `measurementType` and its values in `measurements` to __3 new distinct variables__ (columns) to represent these measurements. \n",
    "\n",
    "To accomplish this conversion we need to do the opposite of `melt()` and use the `pivot()` function or method. The `pivot()` function has the following relevant parameters:\n",
    "\n",
    "- `index`: the column or columns used to make the new DataFrame's index. Choosing this can be crucial as these columns will combine to make unique observations and will also help if we plan on joining with another DataFrame. More importantly, these are the __other columns we want to keep__ in the resulting table. While this is an _optional_ parameter, the default behaviour will use the `index` labels of the `DataFrame`.\n",
    "- `columns`: the column from which you are going to retrieve new column names\n",
    "- `values`: the column(s) to use for populating the new column values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://github.com/uoft-csb-datasci/CSBdatasci_Course_Materials/blob/main/IntroPython/reshaping_pivot.png?raw=true\" width=700>|\n",
    "|:-:|\n",
    "|We reshape our long-format data to a wide-format with the `pivot()` function or method. In the above example, the variable `bar` has 3 value types (A, B, C) that we wish to redistribute as separate variables. The values for these new columns will be redistributed from the `baz` variable. Unfortunately the data in the __zoo__ variable does not get redistributed as part of the process because it was not included in the `index` parameter. This image was provided by the [Pandas online documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/).|\n",
    "\n",
    "First, recall that we can directly call on column names as though they are attributes/properties of the DataFrame. As a sanity check, let's see what our new column names would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the values in measurementType?\n",
    "embryo_long_subset_joined[\"measurementType\"]...\n",
    "\n",
    "# What are the current column names\n",
    "embryo_long_subset_joined.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's use the `.pivot()` method on our joined DataFrame and convert it into a new set of data. To accomplish this task, we'll need a list of the column names we want to _keep_ after the conversion. In this case, we want to retain all of the other columns after conversion. Because our `measurements` column is sandwiched at the beginning by columns we want to retain, we'll have to make a custom list of column positions to capture the set of (0, 2-6).\n",
    "\n",
    "We can use a combination of `.columns` and `np.arange()` to subset the column names. Note, however, that you will want your combined column information to produce a unique set of values for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embryo_long_subset_joined.pivot(\n",
    "    # Set the index using column names we want to keep (0,2,3,4,5,6)\n",
    "    index = embryo_long_subset_joined.columns[np.append(0, np.arange(2,7))],\n",
    "    # We want the column names to come from measurementType\n",
    "    columns = ...,\n",
    "    # The values to populate these columns will come from the measurements column\n",
    "    values = ...\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.7.6 Update/clean your indices with `.reset_index()`\n",
    "\n",
    "From our results with `.pivot()`, you can see that all of the other columns were converted to a secondary kind of index under the `measurementType` category. In order to convert these back to actual columns, we can use the `.reset_index()` method which will help to flatten these out. As a result, we'll also be left with an additional label `measurementType` used to describe all the columns now! To be clear, this is NOT the index column label.\n",
    "\n",
    "To remove this label from all the columns, we can use the `.rename_axis()` method. We'll comment the code out for now so we can see what the `DataFrame` looks like after `.reset_index()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(embryo_long_subset_joined.pivot(\n",
    "    # Set the index using column names we want to keep\n",
    "    index = embryo_long_subset_joined.columns[np.append(0, np.arange(2,7))],\n",
    "    # We want the column names to come from measurementType\n",
    "    columns = \"measurementType\",\n",
    "    # The values to populate these columns will come from the measurements column\n",
    "    values = \"measurements\")\n",
    " \n",
    "    # Turn the indices back into columns\n",
    "    ...\n",
    "\n",
    "    # Use the rename_axis() method to get rid of the extra label\n",
    "#     .rename_axis(mapper=\"\", axis = \"columns\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.8.0 Put your code together now that you've tested it\n",
    "\n",
    "And... that was the last change to `embryo_long_subset_trimmed`. Now that we have the DataFrame in the shape we wanted, let's preprocess the __entire dataset__ by combining all of the code snippets we've generated. \n",
    "\n",
    "Remember we have a few steps, some of them repetitive:\n",
    "1. Convert from wide to long format with `.melt()`\n",
    "2. Remove the `NaN` values from the result and then save that to a variable names `embryo_long_trimmed` (A)\n",
    "3. Take the `experiment` column and split it up based on the underscore characters in each. This will require a:\n",
    "    - `pop()` call to the trimmed data\n",
    "    - `str.split()` on the returned `Series`\n",
    "    - `rename()` on the resulting DataFrame\n",
    "4. Combine the resulting DataFrame to `embryo_long_trimmed` using the `.join()` method.\n",
    "    - This join() step will take the form of `A.join(B)`.\n",
    "5. Then we'll pivot the `measurementType` and `measurements` columns into 3\n",
    "6. Reset the indices to clean up the column headers\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: convert our data to long format and remove NA values\n",
    "embryo_long_trimmed = (embryo_wide\n",
    "                       .melt(id_vars= embryo_wide_subset.columns[0],  # This is our melt step\n",
    "                            var_name=\"experiment\",\n",
    "                            value_name=\"measurements\")\n",
    "                       .dropna(axis = 0, how = 'any') # drop the NaN values after the melt() to the long format\n",
    "                      )\n",
    "\n",
    "# Save your initial joined DataFrame\n",
    "embryo_long_final = (...\n",
    "                     .join( # Join what follows to the trimmed data\n",
    "                         # pop() the experiment column\n",
    "                         embryo_long_trimmed.pop(\"experiment\")  \n",
    "                         \n",
    "                         # split the data into a DataFrame\n",
    "                         .str.split(pat = \"_\", expand = True)\n",
    "                         \n",
    "                         # rename the columns\n",
    "                         .rename(columns = {0:\"date\", 1:\"wormStrain\", 2:\"pathogenStrain\", \n",
    "                                            3:\"pathogenDose\", 4: \"timepoint\", 5:\"measurementType\"})\n",
    "                     )\n",
    "                    )\n",
    "                     \n",
    "# Pivot out the measurementType and measurements columns\n",
    "embryo_long_final = (...(\n",
    "    index = embryo_long_final.columns[np.append(0, np.arange(2,7))],\n",
    "    # We want the column names to come from measurementType\n",
    "    columns = \"measurementType\",\n",
    "    # The values to populate these columns will come from the measurements column\n",
    "    values = \"measurements\")\n",
    "                     \n",
    "    # Reset the indices to be columns\n",
    "    .reset_index()\n",
    "                     \n",
    "    # Remove the \"measurementType\" the column axis label\n",
    "    ...(mapper = \"\", axis = \"columns\")\n",
    ")\n",
    "                     \n",
    "# Take a look at the final result!\n",
    "embryo_long_final.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What can we find out about the final result?\n",
    "embryo_long_final.info()\n",
    "\n",
    "embryo_long_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "As you can see, our final DataFrame has 15,128 observations and 9 columns of which the last 3 represent data measurements ie. dependent variables (`sporesPresent`, `merontsPresent`, and `numEmbryos`) while the others (except `worm.number`) represent metadata information about the conditions of the observation - some of which can be considered independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.9.0 Generate summary information for our data subset\n",
    "\n",
    "Before we wrap up on this data for a bit, let's look deeper at our data see what our data is like. We could have done this with our larger dataset _but_ it would likely be overwhelming and after all of the changes we've made it would not be an accurate description. \n",
    "\n",
    "We can use properties like `.dtypes` to find out the data types for our columns, and the method `.describe()` to produce summary statistics for those columns too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What types of data do we have in each variable\n",
    "embryo_long_final..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.9.1 Alter your column types using the `.astype()` method\n",
    "\n",
    "As we can see above from our call to `dtypes`, all of our meta data is of the type `object` which is essentially a mixed type object or string! All of our measurements are being treated like numeric values (`float64`) and what we want is a mix of booleans (`merontsPresent` and `sporesPresent`) and integers (`numEmbryos`). To remedy this shortcoming we can alter the data types of our columns using a dictionary object of the format `{'colName':'type'}`. \n",
    "\n",
    "Altering this will create a _copy_ of the DataFrame by default. You can alter the `copy` parameter and set it to `False` but unless you know for certain that there are no other important references (aka variables) pointing at this object, you are better off just replacing the reference with a new/updated copy provided by the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame to the original variable\n",
    "embryo_long_final = embryo_long_final.astype({'sporesPresent':'bool', \n",
    "                                              'merontsPresent':'bool', \n",
    "                                              ...})\n",
    "\n",
    "# What are the data types now?\n",
    "embryo_long_final.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.9.2 Use the `.assign()` method to quickly add data columns to the end of your DataFrame\n",
    "\n",
    "One last thing before we move on, the current order of our columns is nearly perfect but we would like to have the `numEmbryos` at the end of our DataFrame instead of between our two boolean variables. To remedy that we could use the `insert()` function but that requires a few parameters:\n",
    "\n",
    "- `loc`: the position you wish to insert your column at\n",
    "- `column`: the name of the column you will insert\n",
    "- `value`: the column information you are inserting - usually a series or similar data structure.\n",
    "\n",
    "As a bonus, this will perform the change on the DataFrame _inplace_ so there's no need to reassign the result.\n",
    "\n",
    "Instead of using that method, however, since we want to insert at the end of our DataFrame, we can use the `.assign()` method where we simply name our column and assign the values. This will append the new column to the end of the DataFrame BUT it will return a new DataFrame as a result - which means we must save it! \n",
    "\n",
    "In either case, we must use the `.pop()` method to also remove `numEmbryos` before adding it back to the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the numEmbryos column to the end of embryo_long_final\n",
    "embryo_long_final = embryo_long_final.assign(numEmbryos = ...)\n",
    "\n",
    "# What does our DataFrame look like now?\n",
    "embryo_long_final.head()\n",
    "\n",
    "# Make a copy for later (in the comprehension question)!\n",
    "embryo_long_final_copy = embryo_long_final.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.9.3 Use the `.describe()` method to create summary statistics of our DataFrame\n",
    "\n",
    "Now we can take a moment to create some summary statistics on our DataFrame! Let's try out the `.describe()` method which will return a `DataFrame` with information like the count, mean, and standard deviation of individual data columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics on our data\n",
    "embryo_long_final..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.9.3.1 The `describe()` method works for numeric information AND non-numeric data\n",
    "\n",
    "From above, we see that our summary statistics do not include columns like the `date`, `wormStrain`, `pathogenStrain`, `pathogenDose`, etc. Given that these are character-based or booleans, the `describe()` method cannot generate any summary statistics for these columns. As a default, the method will only return summary information on __numeric__ column data.\n",
    "\n",
    "If, however, we __explicitly__ provide only non-numeric column(s) to `describe()` we are returned a truncated or altered version of the output. You can provide a single or multiple character columns with similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Provide a single column\n",
    "embryo_long_final.iloc[...].describe() # We get a count of how many non-na values are in the column\n",
    "\n",
    "# Provide multiple columns\n",
    "embryo_long_final.iloc[...].describe()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "From `.describe()` we can see how many total values there are, the number of unique entries, what the most frequent entry in the column is and how often it occurs.\n",
    "\n",
    "### 3.9.4 Use the `.unique()` method to examine values in your DataFrame\n",
    "\n",
    "As we can see from above, there are 26 unique `wormStrain` values. We can see what they are using the `.unique()` method on a column from our DataFrame. This is a quick way to see the nature of values in your dataset as well. Note that this will return a `np.array` as a result so we'll cast it to a `list` for viewing simplicity. Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the subsite information\n",
    "embryo_long_final[\"wormStrain\"].unique().tolist()\n",
    "\n",
    "# unique() will return an array, which we cast as a list!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.9.5 Retrieve your DataFrame as a nested `np.Array` with the `.values` property\n",
    "\n",
    "Another way to access portions or all of the values in your DataFrame, you can use the `.values` property. This will return a `Numpy` representation of the DataFrame. This is typically an array of arrays which you can also subset with multi- or chained-indexing (where appropriate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull down the values from our DataFrame as a 2D array\n",
    "embryo_long_final.values  # easier to see when we have smaller datasets. \n",
    "\n",
    "# Subset the values into a 2D array of objects\n",
    "embryo_long_final.values...\n",
    "\n",
    "# Return a single 1D array of objects\n",
    "embryo_long_final.values..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Section 3.10.5 Comprehension Question:</b> As we've seen from the above code examples there are many paths to the same result. Using the following code cells, demonstrate 2 different methods you can use to remove the <b>sporesPresent</b> and <b>merontsPresent</b> variables from the provided DataFrame. You can choose to use method chaining if needed. Avoid creating throw-away intermediate variables and use the <b>.info()</b> method to check your results!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprehension answer code 3.10.5a\n",
    "# Remove BOTH the sporesPresent and merontsPresent columns in 2 different ways\n",
    "embryo_comprehension1 = embryo_long_final_copy.copy()\n",
    "\n",
    "# Method 1: remove sporesPresent and merontsPresent from embryo_comprehension1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprehension answer code 3.10.5b\n",
    "embryo_comprehension2 = embryo_long_final_copy.copy()\n",
    "\n",
    "# Method 2: remove sporesPresent and merontsPresent from embryo_comprehension2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://github.com/camok/CSB_Course_Materials/blob/main/IntroPython/data-wrangling-ooooooh-yyyeeeeaaahhh.jpg?raw=true\" width=400>|\n",
    "|:--:|\n",
    "|One file down. One more to go.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.10.0 Data wrangling `embryo_metadata`\n",
    "\n",
    "While our measurement data has been tidied, the additional metadata regarding the experimental conditions in our dataset has been separated into the `embryo_metadata` set that we explored earlier. \n",
    "\n",
    "Before we can unite our metadata with the measurement data, we must prepare it as well into a properly formatted long-format dataset. While this doesn't fit with our previously wrangled data, it will soon make more sense to us.\n",
    "\n",
    "### 3.10.1 Import and explore `embryo_metadata`\n",
    "\n",
    "Recall our general steps:\n",
    "\n",
    "1. __Import and explore the file to identify issues that might be present with your data. What general problems may arise as you wrangle this dataset?__\n",
    "\n",
    "2. Create a subset of your data. Sometimes you are working with VERY large datasets. The process of wrangling a large file can take a lot of processing power and time. You don't want to spend your time experimenting on the entire dataset to come up with a customized procedure to wrangle it.\n",
    "\n",
    "3. Remove unwanted data. Some columns or rows may be irrelevant to your analysis or unuseable due to missing data discovered in step 1. Eliminate these as needed.\n",
    "\n",
    "4. Convert your dataset into long format to help conform with tidy data principles.\n",
    "\n",
    "Remember that the ultimate goal is to join the `embryo_metadata` and `embryo_long_final` datasets. Let's start by inspecting the metadata set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our data again (just in case)\n",
    "embryo_metadata = pd.read_csv(\"infection_meta.csv\") \n",
    "\n",
    "# Take a quick look at the DataFrame\n",
    "embryo_metadata.head()\n",
    "\n",
    "# Get a general sense of the column information\n",
    "embryo_metadata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Recall that we've already checked for `NaN` values within our data and found them within 3 columns. The `.info()` method also confirms that we have `NaN` values in `Time plated`, `Time Incubated` and `Location`. For our purposes, these variables are miscellaneous data and won't affect anything later on so we can keep this as it is or choose to remove it.\n",
    "\n",
    "You'll notice that our `experiment` column looks nearly identical to the column names in our _original_ `embryo_data_wide` dataset except it lacks a `<measurementType>` as the last part of each label. These column names in the original dataset have since been split into 6 data columns. This data acts as a set of unique identifier information. In this case we'll follow suit and split the `experiment` column out into five new ones.\n",
    "\n",
    "Note also that we have 276 entries in the `experiment` column. Recall our original measurement dataset `embryo_data_wide` had 900 data columns which translates to 300 experimental conditions. That means there is a discrepancy between the metadata (276 experiments) and our measurement data (300 experiments). We'll have to consider this carefully when combining our datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.10.2 Remove unwanted data and convert columns as needed\n",
    "\n",
    "We've taken a look at the structure and nature of our dataset and we're ready to move onto the next steps. Given the size of the dataset is relatively small, we can forgo step 2 and go into step 3.\n",
    "\n",
    "1. Import and explore the file to identify issues that might be present with your data. What general problems may arise as you wrangle this dataset? $\\checkmark\\checkmark$\n",
    "\n",
    "2. Create a subset of your data. Sometimes you are working with VERY large datasets. The process of wrangling a large file can take a lot of processing power and time. You don't want to spend your time experimenting on the entire dataset to come up with a customized procedure to wrangle it.\n",
    "    - The dataset is only 276 x 29 so no need to subset.\n",
    "    \n",
    "3. __Remove unwanted data. Some columns or rows may be irrelevant to your analysis or unusable due to missing data discovered in step 1. Eliminate these as needed.__\n",
    "    - The `Time plated`, `Time Incubated` and `Location` values aren't very important so we can remove them to save possible headaches further down the road.\n",
    "\n",
    "4. Convert your dataset into long format to help conform with tidy data principles.\n",
    "\n",
    "Let's remove those 3 columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 3 columns and check the information. Make the change inplace to save on syntax\n",
    "embryo_metadata.drop(columns = ['Time plated', 'Time Incubated', 'Location'], ...)\n",
    "\n",
    "embryo_metadata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "We've removed the 3 columns using the `.drop()` method so now we're on to splitting up the `experiment` column. This action falls somewhere into step 4 since we don't need to convert from wide to long. However, if we take a close look at the data, we see that these variables ALREADY exist but under different column names:\n",
    "\n",
    "|`experiment` column|pre-existing `metadata` column| type|index|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|date|Fixing Date|int64|19|\n",
    "|wormStrain|Worm_strain|object|5|\n",
    "|pathogenStrain|Spore Strain|object|7|\n",
    "|pathogenDose|Total Spores (M)|object|10|\n",
    "|timepoint|timepoint|object|17|\n",
    "\n",
    "More importantly, you might recall from previous sections, that the data types are a little different from our `embryo_long_final` data where all of these variables are treated as strings (objects!). Let's convert `Fixing Date` into the correct data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10.3 Split columns to conform to tidy data format\n",
    "\n",
    "1. Import and explore the file to identify issues that might be present with your data. What general problems may arise as you wrangle this dataset? $\\checkmark\\checkmark$\n",
    "\n",
    "2. Create a subset of your data. Sometimes you are working with VERY large datasets. The process of wrangling a large file can take a lot of processing power and time. You don't want to spend your time experimenting on the entire dataset to come up with a customized procedure to wrangle it. $\\checkmark\\checkmark$\n",
    "    - The dataset is only 276 x 29 so no need to subset.\n",
    "\n",
    "3. Remove unwanted data. Some columns or rows may be irrelevant to your analysis or unuseable due to missing data discovered in step 1. Eliminate these as needed. $\\checkmark\\checkmark$\n",
    "    - The `Time plated`, `Time Incubated` and `Location` values aren't very important so we can remove them to save possible headaches further down the road.\n",
    "\n",
    "4. __Convert your dataset into long format to help conform with tidy data principles.__\n",
    "    - Convert the `experiment` column into five new columns? Not needed!\n",
    "    - Convert the data type for `Fixing Date` and rename columns as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alter our Fixing Date dtype\n",
    "embryo_metadata = embryo_metadata.astype(...)\n",
    "\n",
    "# Check the resulting columns\n",
    "embryo_metadata.iloc[:, [19,5,7,10,...]].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Now all that remains is to rename our columns for a smoother integration of the data. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Why bother renaming our columns?:</b> In our following steps when using the <b>merge</b> function, if we don't rename the columns, we will actually end up with duplicated information. We don't need a version from each column so by giving them identically matched names, pandas will not keep any extra columns!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embryo_metadata.rename(columns = {'Fixing Date':'date',\n",
    "                        'Worm_strain':'wormStrain',\n",
    "                        'Spore Strain':'pathogenStrain',\n",
    "                        ...}, \n",
    "                       inplace = True\n",
    "                      )\n",
    "\n",
    "# Check the resulting columns\n",
    "embryo_metadata.iloc[:, [19,5,7,10,17]].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Looks like the datatypes for our common columns is as expected and the names have been corrected. That means we are ready to go ahead and join our `embryo_metadata` with our `embryo_long_final` data!\n",
    "\n",
    "# 4.0.0 Merging data\n",
    "\n",
    "Into the final stretch, now that we have our datasets in a tidy format, it is time to merge it together. This can also be referred to as joining our data. In order to successfully do so, we need ___at least___ one column from each `DataFrame` that we can use as a reference to combine the data. These columns can also be referred to as ___keys___.\n",
    "\n",
    "Previously when we used the `.join()` method, we were automatically joining on the `index` of each DataFrame because they shared the original index with a 1:1 representation in each. \n",
    "\n",
    "In our current case, we already know there are more experimental observations in our original data (300 to be specific) compared to our metadata information (276). We also don't share `index` values since these were independent datasets. We do, however, share a number of common columns as we saw from above! \n",
    "\n",
    "In our case, we will use this unique set of 5 identifiers as a ___key___ to join the two datasets by matching on these values. Programs like Python can do a good job of merging tables for us versus using a copy-paste method in spreadsheet software. You can mess up your dataset and ultimately negatively affect your results. In our case, our __data:metadata__ relationship has many data observations linked to single metadata entries.\n",
    "\n",
    "There are several ways to join two datasets with the Pandas package of which we've seen one:\n",
    "\n",
    "- `join()`: a DataFrame method to join multiple DataFrame objects by index at once\n",
    "- `concat()`: Use various methods and options to concatenate multiple DataFrame objects\n",
    "- `merge()`: combine two DataFrames into a single DataFrame by describing multiple relevent keys/columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmp_metadata.join?\n",
    "# pd.concat?\n",
    "# pd.merge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our understanding of the data, we want to combine on multiple sets of columns rather than a single index (like we did with `.join()`. Therefore we'll go with the `merge()` function to help us out. Recall that our two dataframes share the following columns in common: `date`, `wormStrain`, `pathogenStrain`, `pathogenDose`, and `timepoint`.\n",
    "\n",
    "***\n",
    "## 4.1.0 Use `merge()` to combine our wrangeld subsets\n",
    "\n",
    "Let's take a closer look at the `merge()` function which has some of the following parameters\n",
    "\n",
    "- `left`: a DataFrame you wish to merge _into_. Its data will appear first in the resulting DataFrame.\n",
    "- `right`: a DataFrame or a named Series that you'll be merging with.\n",
    "- `how`: the type of merge you want to perform. Options are `left`, `right`, `outer`, and `inner` (default).\n",
    "- `on`: a list of shared column names that you want to match across both the left and right datasets.\n",
    "- `left_on`: the column or index level names to join on in the left DataFrame.\n",
    "- `right_on`: the column or index level names to join on in the right DataFrame.\n",
    "- `left_index`: Use the index from the left DataFrame as the join key if `True`. Defaults to `False` and is overridden by `left_on` parameter.\n",
    "- `right_index`: Use the index from the right DataFrame as the join key if `True`. Defaults to `False` and is overridden by `right_on` parameter.\n",
    "\n",
    "Let's combine our two subsets now `embryo_long_final` and `embryo_metadata` now that we've ensured they share the same keys. Note that this isn't _strictly_ necessary. Each join type in the `how` parameter deals with merging observations based on keys in a different manner. \n",
    "\n",
    "| Join type | Description |\n",
    "| :- | :- |\n",
    "| left | Keep everything from the left DataFrame and only merge intersecting keys from the right DataFrame. Missing keys are filled with `NaN`s|\n",
    "| right | Keep everything from the right DataFrame and only merge intersecting keys from the left DataFrame. Missing keys are filled with `NaN`s |\n",
    "| outer | AKA a full outer join, creates the union of keys and merges based on these. Unmatched cases are filled with `NaN`s |\n",
    "| inner | Creates the intersection of the keys from left and right and merges based on these |\n",
    "\n",
    "The next step is to join the two datasets by common columns or a shared index. If we decide to do this by column, the columm(s) that we use as a key __must be type-matched across both datasets__. If we decide to go with the indices as key for the merge, we just set the `merge()` arguments `left_index` and `right_index` to `True`. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Check it out:</b> Read more about the wonderful world of merging <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\">here</a>.\n",
    "</div>\n",
    "\n",
    "Before we merge our data, let's take one last look at the sizes of these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What size is our measurement dataset?\n",
    "embryo_long_final.shape\n",
    "\n",
    "# What size is our metadata set?\n",
    "embryo_metadata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's merge the embryo metadata into our actual data set based on their shared index values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets based on matching the correct columns between each\n",
    "# Perform an 'inner' merge so only intersecting data remains.\n",
    "\n",
    "embryo_merged = pd.merge(left = ..., right = ...,\n",
    "                         how = ...,\n",
    "                         on = ...\n",
    "                        )\n",
    "\n",
    "embryo_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Our merge has generated a new dataframe `embryo_merged` which is an intersection between the two datasets resulting in a dataframe of 11149 observations across 30 variables!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Section 4.1.0 Comprehension Question:</b> Notice the information from above? We started with 15,128 rows of data in our long-format <b>embryo_long_final</b> DataFrame. After merging we have only 11,149 rows. Is there an alternative set of merge parameters which will retain our original data from both DataFrames regardless of having the matching metadata available? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehension answer 4.1.0:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4.1.1 You will always be wrangling data in one way or another\n",
    "\n",
    "Hopefully you got a hang of what data wrangling (prepocessing) is about and why it often represents 80 to 90% of your data analysis time. The next time you start planning your experiment, it is advisable to also plan ahead on how to record your data so you do not have too spend too long formating it. \n",
    "\n",
    "As a bonus, if you are mining data or repeating experiments and always getting the same kind of data format, then once you have built these kinds of scripts you can reuse them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4.2.0 Write your data to file with the `to_csv()` method\n",
    "\n",
    "The Pandas DataFrame class has a built-in method for exporting our data to a various file formats like the comma- or tab-separated value formats. We can actually just use the `to_csv()` method for both file types because we can alter some of the parameters during the call:\n",
    "\n",
    "- `path_or_buf`: a file path or object to write to. In this case just stick with a path.\n",
    "- `sep`: a string of length \"1\" denoting how you want to delimit your values ie `,`, `;`, `\\t`.\n",
    "- `index`: a boolean to write out the row names as a column (default is `True`).\n",
    "- `header`: write out the column names (default is `True`). You can also pass a list of strings to overwrite the column names.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Read more:</b> You can go through more parameters by checking out the <b>?pd.DataFrame.to_csv</b> command or going to the pandas documentation <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html\">here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?pd.DataFrame.to_csv\n",
    "\n",
    "# We'll use a small dataset to write to files\n",
    "\n",
    "# Write an actual csv file\n",
    "...to_csv(..., sep = ..., index = False)\n",
    "\n",
    "# \\t for tab-separated value. Notice that the function name is still to_csv\n",
    "# Setting index=True will create an extra column (without a header) on the left-hand side of your file\n",
    "embryo_long_final.to_csv('embryo_data_long.tsv', sep = ..., index = True) \n",
    "\n",
    "print('Done writing files!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4.2.1 Write to an excel file with the `to_excel()` method\n",
    "\n",
    "Similar in idea to the `to_csv()` method, this will allow us to directly write our DataFrame objects as Microsoft Excel files. We just need to set the proper parameters:\n",
    "\n",
    "- `excel_writer`: the file path where we want to send our data, or an `ExcelWriter` object.\n",
    "- `na_rep`: how we want to present missing data (`''`, empty quotes are the default).\n",
    "- `index`: a boolean to write out the row names as a column (default is `True`).\n",
    "- `header`: write out the column names (default is `True`). You can also pass a list of strings to overwrite the column names.\n",
    "- `sheet_name`: If not supplied, 'Sheet1' will be used by default.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Read more:</b> You can go through more excel-writing parameters by checking out the <b>?pd.DataFrame.to_excel</b> command or going to the pandas documentation <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html\">here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ?pd.DataFrame.to_excel\n",
    "\n",
    "# We'll just use some small datasets to write to an excel file\n",
    "embryo_merged.to_excel('embryo_merged.xlsx', index=...)\n",
    "\n",
    "print('Done writing excel files!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4.2.2 Write multiple `DataFrames` to an excel file with the `pd.ExcelWriter()` function\n",
    "\n",
    "Unlike using a method on a single `DataFrame` object, if we want to save multiple `DataFrames` to a single Excel file, we'll need to first create a connection to a file. We do this with `pd.ExcelWriter()` which requires us to specify some of the following parameters:\n",
    "\n",
    "- `path`: the path to the location where you want to save the file - including the file name.\n",
    "- `engine`: the engine to use for writing. The default is \"xlsxwriter\" for Excel files, but it can also write to Open Office with the \"odswriter\"\n",
    "- `mode`: whether or not you wish to `w` write to a new file, or `a` append to a pre-existing file.\n",
    "\n",
    "From there, we can supply this `ExcelWriter` in the `.to_excel()` method for each `DataFrame` we are writing but we'll want to be sure to use the `sheet_name` parameter at that time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write multiple DataFrames to an excel file\n",
    "\n",
    "# Create the writer object\n",
    "writer = ...(path=..., engine = ..., mode = \"w\")\n",
    "\n",
    "# Write each dataframe to the file\n",
    "embryo_merged.to_excel(writer, sheet_name = \"embryo_merged\")\n",
    "embryo_long_final.to_excel(writer, sheet_name = \"embryo_long\")\n",
    "embryo_wide.to_excel(writer, sheet_name = \"embryo_wide_original\")\n",
    "\n",
    "# Close the connection\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 5.0.0 Class summary\n",
    "\n",
    "That's our third class on Python! You've made it through and we've learned about working with data files:\n",
    "\n",
    "1. The tidy data philosophy.\n",
    "2. Reading in `.xlsx` and `.csv` file formats.\n",
    "3. Strategies for data wrangling - subsets, removing data, merging tables.\n",
    "4. Writing your wrangled data to a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5.1.0 Submit your completed skeleton notebook (2% of final grade)\n",
    "\n",
    "At the end of this lecture a Quercus assignment portal will be available to submit your completed skeletons from today (including the comprehension question answers!). These will be due one week later, before the next lecture. Each lecture skeleton is worth 2% of your final grade but a __bonus 0.5%__ will also be awarded for submissions made within 24 hours from the end of lecture (ie 1600 hours the following day). \n",
    "\n",
    "1. From the Jupyter Notebook, select `File> Save Notebook` or use `ctrl + S`\n",
    "2. Return to the Jupyter folder page (ie click on the top-left \"jupyterhub\" icon)\n",
    "3. Locate your .ipynb file and click on the check box to its left side.\n",
    "4. If it is still active it will have a green icon beside its name. Above the file list will be option to `Shut Down`. After clicking on this, you may re-select the file and above the file list choose `Download` to save to your local hard drive.\n",
    "5. Upload your .ipynb file to the appropriate Quercus assignment portal.\n",
    "\n",
    "|<img src=\"https://github.com/uoft-csb-datasci/CSBdatasci_Course_Materials/blob/main/IntroPython/save_ipynb.png?raw=true\" width=700>|\n",
    "|:--:|\n",
    "|A sample screen shot for saving your completed skeleton notebooks|\n",
    "\n",
    "## 5.2.0 Post-lecture DataCamp assessment (5% of final grade)\n",
    "\n",
    "Soon after the end of each lecture, a homework assignment will be available for you in [DataCamp](https://www.datacamp.com/).  Your assignment is to complete chapters 1-3 of Data Manipulation with pandas (Transforming Data, 950 possible points; Aggregating Data, 1300 possible points; and Slicing and Indexing, 1250 possible points). This is a pass-fail assignment, and in order to pass you need to achieve a least 2625 points (75%) of the total possible points. Note that when you take hints from the DataCamp chapter, it will reduce your total earned points for that chapter.\n",
    "\n",
    "In order to properly assess your progress on DataCamp, at the end of each chapter, please print a PDF of the summary. You can do so by following these steps:\n",
    "\n",
    "1. Navigate to the **`Learn`** section along the top menu bar of DataCamp. This will bring up a sidebar menu where you can access the various courses you have been assigned under **`Assignments`** or you can scroll down on the page to find the **`My Assignments`** section. Click on the relevant chapter or course and this may bring you back to within the course itself. \n",
    "2. You should now be on a Course Summary page (See figure below). You can expand each __chapter__ of the course by clicking on the **`VIEW CHAPTER DETAILS`** link. ___DO THIS FOR ALL COMPLETED SECTIONS ON THE PAGE!___ \n",
    "3. Select the visible text in the course Window (not from any top or side menus) using a click and drag technique. Using something like __Ctrl+A will not properly select the course text__. \n",
    "4. Print the page from your browser menu and make sure you have \"Selection Only\" from the Settings options on the Print dialogue. Save as a single PDF. Note: if you don't select the text area correctly (at least in Google Chrome) you may not be able to print the full page. Your results should look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"https://github.com/uoft-csb-datasci/CSBdatasci_Course_Materials/blob/main/IntroPython/DataCamp.example.png?raw=true\" width=700>|\n",
    "|:--:|\n",
    "|A sample screen shot for one of the DataCamp assignments. You'll want to try and print off a single PDF of this section from **Learn > My Assignments**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to take several screenshots if you cannot print it all in a single try. Submit the file(s) or a combined PDF for the homework to the assignment portal on Quercus. By submitting your scores for each section, and chapter, we can keep track of your progress, identify knowledge gaps, and produce a standardized way for you to check on your assignment \"grades\" throughout the course.\n",
    "\n",
    "You will have until 0959 hours on Tuesday, January 27th to submit your assignment (right before the next lecture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5.3.0 Acknowledgements\n",
    "\n",
    "__Revision 1.0.0__: materials prepared by Oscar Montoya, M.Sc. _Bioinformatician, Education and Outreach, CAGEF._\n",
    "\n",
    "__Revision 1.1.0__: edited and prepared for __CSB1021H S LEC0140__, 06-2021 by Calvin Mok, Ph.D. _Bioinformatician, Education and Outreach, CAGEF._\n",
    "\n",
    "__Revision 1.2.0__: edited and prepared for __CSB1021H S LEC0140__, 01-2022 by Calvin Mok, Ph.D. _Bioinformatician, Education and Outreach, CAGEF._\n",
    "\n",
    "__Revision 1.2.1__: edited and prepared for __CSB1021H S LEC0140__, 01-2023 by Calvin Mok, Ph.D. _Bioinformatician, Education and Outreach, CAGEF._\n",
    "\n",
    "__Revision 1.2.2__: edited and prepared for __CSB1021H S LEC0140__, 01-2024 by Calvin Mok, Ph.D. _Bioinformatician, Education and Outreach, CAGEF._\n",
    "\n",
    "__Revision 1.2.3__: edited and prepared for __CSB1021H S LEC0140__, 01-2025 by Calvin Mok, Ph.D. _Bioinformatician, Education and Outreach, CAGEF._\n",
    "\n",
    "__Revision 1.2.4__: edited and prepared for __CSB1021H S LEC0140__, 01-2026 by Calvin Mok, Ph.D. _Bioinformatician, Education and Outreach, CAGEF._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5.4.0 Resources\n",
    "\n",
    "- https://pandas.pydata.org/doc\n",
    "\n",
    "- https://www.datacamp.com\n",
    "\n",
    "- Pandas cheatsheets: \n",
    "    - https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf \n",
    "    - http://datacamp-community-prod.s3.amazonaws.com/dbed353d-2757-4617-8206-8767ab379ab3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 6.0.0 Appendix 1: the `openpyxl` package\n",
    "\n",
    "## 6.1.0 Use the `openpyxl` package to open excel files\n",
    "\n",
    "In earlier years the `xlrd` package could be used to read excel spreadsheet files of the format __.xls__. However, in the past decade Microsoft has focused on the __.xlsx__ format which the `xlrd` package supported but development on the `xlrd` package officially ended support for anything but the __.xls__ format as of December 2020.\n",
    "\n",
    "Now the official recommended package is `openpyxl` to open the more complex excel files. You can also use this package to make and save excel spreadsheet files so it's a good one to keep in your toolbox.\n",
    "\n",
    "Let's start by importing the file `miscellaneous.xlsx` from the `data` directory by calling on the `load_workbook()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openpyxl\n",
    "\n",
    "import openpyxl as opxl\n",
    "\n",
    "# Open the correct file since we are in the data directory already\n",
    "wb = opxl.load_workbook('infection_data_all.xlsx')\n",
    "\n",
    "# What kind of object have we created?\n",
    "type(wb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 6.1.1 Look at sheet names with `.sheetnames` property\n",
    "\n",
    "Now that we've generated this `Workbook` object, we can look at some of it's properties, like the names of the various sheets using the `.sheetnames` property. Remember this will be holding data in a complex hierarchy where we'll have to break down the information into something like a `DataFrame` object. \n",
    "\n",
    "From there we can access individual sheets using the `[\"sheetname\"]` indexing operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What are the sheetnames of our excel workbook\n",
    "wb.sheetnames\n",
    "\n",
    "pathogen_sheet = wb[\"microsporidia_info\"]\n",
    "\n",
    "type(pathogen_sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 6.1.2 Access the values of a worksheet with the `.values` property\n",
    "\n",
    "Now that we have pulled out a `Worksheet` object, we have to imagine that there are values there for us to use. The way to access this information on the various cells is with the `.values` property. We'll see, however, that it's not exactly a data structure like we imagine it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get a copy of the worksheet data\n",
    "pathogen_data = wb[\"microsporidia_info\"].values\n",
    "\n",
    "# Convert the data using Pandas!\n",
    "pd.DataFrame(pathogen_data)\n",
    "\n",
    "# What kind of object is returned when we call on the values of the sheet\n",
    "print(type(pathogen_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 6.1.3 Iterate through a generator with the `next` command\n",
    "\n",
    "As you can see from above, we managed to convert our worksheet data into a DataFrame but it wasn't exactly a _perfect_ conversion. Looking at the original object we get from the `.values` property, we get a `generator` object in return.\n",
    "\n",
    "A `generator` is a special function that can make its way through an iterable object (like a list or dictionary) but can return a sequence of values and not just a single value. It will also __yield__ it's execution wherever it was in the iterable object and return that state to you in another `generator` object. Sounds confusing right? \n",
    "\n",
    "Simply put, this kind of function serves as an iterator for moving through the rows of our sheet value data one at a time using the `next()` command. This is helpful to know because looking closer at our DataFrame we see that the header information was deposited into the first row of our DataFrame rather than as column names. There are a few ways to remedy this but one way is to properly iterate through our generator with the `next()` command.\n",
    "\n",
    "By default the `next()` command of our `Worksheet.values` generator will return a row from our values. Once it gets to the end of the structure, it will return a `StopIeration` error when calling `next()`.\n",
    "\n",
    "Let's take a look ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Go to the next line in our values\n",
    "next(pathogen_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we get the stop iteration? When we assigned it to a DataFrame above, it iterated through the whole `.values` generator and now there's nothing else to squeeze from it. We'll have to start over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-assign the values data. \n",
    "pathogen_data = wb[\"microsporidia_info\"].values\n",
    "\n",
    "# Iterate through some of those rows\n",
    "next(pathogen_data)\n",
    "\n",
    "# What type of object is this returning\n",
    "type(next(pathogen_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 6.1.4 The `next()` function returns a tuple\n",
    "\n",
    "That's right, our good friend the `tuple` is back and therefore we can slice it with the `[]` operators. That means we can pull out specific \"columns\" that we're interested in from each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get an entire row\n",
    "next(pathogen_data)[:]\n",
    "\n",
    "# This grabs the entire row too\n",
    "next(pathogen_data)[0:]\n",
    "\n",
    "# Slice the next row a little\n",
    "next(pathogen_data)[-5:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 6.1.5 Use `next()` to extract a header for a DataFrame\n",
    "\n",
    "Now that we know the inner workings a little more, we can grab the first row from the `Worksheet.values` generator and keep that for our columns. Then we can pass the rest of the generator to the initialization of the `DataFrame`. Let's see that in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Re-assign the values data. \n",
    "pathogen_data = wb[\"microsporidia_info\"].values\n",
    "\n",
    "# Grab the column names from the first \"tuple\" in our generator\n",
    "colNames = next(pathogen_data)[0:]\n",
    "\n",
    "# Now we send the rest of the generator to be made into a DataFrame\n",
    "pathogens_df = pd.DataFrame(pathogen_data, columns = colNames)\n",
    "\n",
    "# Let's view some of the DataFrame\n",
    "pathogens_df.head()\n",
    "\n",
    "pathogens_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## The Center for the Analysis of Genome Evolution and Function (CAGEF)\n",
    "\n",
    "The Centre for the Analysis of Genome Evolution and Function (CAGEF) at the University of Toronto offers comprehensive experimental design, research, and analysis services in microbiome and metagenomic studies, genomics, proteomics, and bioinformatics.\n",
    "\n",
    "From targeted DNA amplicon sequencing to transcriptomes, whole genomes, and metagenomes, from protein identification to post-translational modification, CAGEF has the tools and knowledge to support your research. Our state-of-the-art facility and experienced research staff provide a broad range of services, including both standard analyses and techniques developed by our team. In particular, we have special expertise in microbial, plant, and environmental systems.\n",
    "\n",
    "For more information about us and the services we offer, please visit <https://www.cagef.utoronto.ca/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/uoft-csb-datasci/CSBdatasci_Course_Materials/blob/main/IntroR/CAGEF_new.png?raw=true\" width=700></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
